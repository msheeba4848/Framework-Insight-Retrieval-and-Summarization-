{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f90a304-074b-4274-90e8-e70eccacb113",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Saving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee27799-49c2-46e5-ae93-0f38b7de23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1494b2d-1ae8-4b33-b138-4ccf37cc4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81995c7c-f547-4861-8bc9-6b306d6a79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date(date):\n",
    "    \"\"\"Standardize date to YYYY-MM-DD format.\"\"\"\n",
    "    try:\n",
    "        if len(date) == 4:  # Year only\n",
    "            return pd.to_datetime(f\"{date}-01-01\").strftime(\"%Y-%m-%d\")\n",
    "        elif len(date) == 7:  # Year and month\n",
    "            return pd.to_datetime(f\"{date}-01\").strftime(\"%Y-%m-%d\")\n",
    "        else:  # Full date\n",
    "            return pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3065f4b-cdf6-47d7-9b30-109ac6ea04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_metadata(json_dir, metadata_file):\n",
    "    \"\"\"Preprocess raw JSON files and save metadata for FAISS.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(json_dir, file_name), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                row = {\n",
    "                    \"file\": file_name,\n",
    "                    \"name\": preprocess_text(data.get(\"name\", \"\")),\n",
    "                    \"abbreviation\": preprocess_text(data.get(\"name_abbreviation\", \"\")),\n",
    "                    \"decision_date\": standardize_date(data.get(\"decision_date\", \"\")),\n",
    "                    \"text\": \" \".join(opinion.get(\"text\", \"\") for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Save processed metadata\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49001734-d61e-41cd-8d5d-ea36fb9ec02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to data/metadata_faiss.json\n"
     ]
    }
   ],
   "source": [
    "json_dir = \"json/\"  # Folder containing raw JSON files\n",
    "metadata_file = \"data/metadata_faiss.json\"  # Metadata file for FAISS\n",
    "preprocess_and_save_metadata(json_dir, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a791f-0551-49c0-b9c3-9c24f7c204eb",
   "metadata": {},
   "source": [
    "## 2. Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dc5685-be3b-41cf-9cba-126f1475a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177b00bc-1704-4642-a9a3-14d693829ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Generate embeddings for text.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9db511-1399-4f6e-a92b-1ff0090f2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(metadata_file, index_file):\n",
    "    \"\"\"Create a FAISS index from metadata.\"\"\"\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Generate embeddings for each text\n",
    "    embeddings = [embed_text(entry[\"text\"]) for entry in metadata]\n",
    "    embeddings = np.vstack(embeddings)  # Combine embeddings into a matrix\n",
    "\n",
    "    # Create and save FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_file)\n",
    "    print(f\"FAISS index saved to {index_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8a30d4-57e8-45fc-9472-5a9b756bc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-14 03:27:02.063169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to data/legal_cases_index.faiss\n"
     ]
    }
   ],
   "source": [
    "create_faiss_index(\"data/metadata_faiss.json\", \"data/legal_cases_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a94be-7c6a-4564-91d4-5723210bb26e",
   "metadata": {},
   "source": [
    "## 3. Interactive Query System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99657d06-f22d-4040-b19f-988cc4e032cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Using cached rapidfuzz-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Using cached rapidfuzz-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af2942d4-58ab-40d4-937c-3c9d0af68323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8efbf890-f558-4c04-b36e-c39564de96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "def is_similar(a, b, threshold=80):\n",
    "    \"\"\"Check if two strings are similar using fuzz.partial_ratio.\"\"\"\n",
    "    return fuzz.partial_ratio(a, b) > threshold\n",
    "\n",
    "def handle_partial_matches(query, metadata, threshold=80):\n",
    "    \"\"\"Retrieve and rank partial matches.\"\"\"\n",
    "    query_normalized = preprocess_query(query)\n",
    "    results = []\n",
    "\n",
    "    for entry in metadata:\n",
    "        name = preprocess_query(entry.get(\"name\", \"\"))\n",
    "        abbreviation = preprocess_query(entry.get(\"abbreviation\", \"\"))\n",
    "        text = preprocess_query(entry.get(\"text\", \"\"))\n",
    "\n",
    "        if fuzz.partial_ratio(query_normalized, name) > threshold or \\\n",
    "           fuzz.partial_ratio(query_normalized, abbreviation) > threshold or \\\n",
    "           fuzz.partial_ratio(query_normalized, text) > threshold:\n",
    "            results.append(entry)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23460511-05a6-469a-9f97-8423c18a579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = query.strip().lower()\n",
    "    query = re.sub(r'[^\\w\\s-]', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = [\n",
    "        'what about', 'can you', 'please', 'show me', 'find', \n",
    "        'search for', 'give me', 'how about', 'tell me about',\n",
    "        'what is', 'on', 'the case on', 'case from', 'is there a case'\n",
    "    ]\n",
    "    for filler in filler_words:\n",
    "        query = re.sub(r'\\b' + re.escape(filler) + r'\\b', '', query)\n",
    "\n",
    "    # Extract date if present in the query\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', query)\n",
    "    if match:\n",
    "        return match.group(0)  # Return the date in YYYY-MM-DD format\n",
    "\n",
    "    return query.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e9059bd-3fff-4560-a525-c00a5f8afefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(user_query, index, metadata, k=5):\n",
    "    \"\"\"Query FAISS index and return top results with partial matching.\"\"\"\n",
    "    query_embedding = embed_text(user_query)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = []\n",
    "\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        metadata_entry = metadata[idx]\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"file\": metadata_entry.get(\"file\", \"Unknown\"),\n",
    "            \"name\": metadata_entry.get(\"name\", \"Unknown\"),\n",
    "            \"abbreviation\": metadata_entry.get(\"abbreviation\", \"Unknown\"),\n",
    "            \"text\": metadata_entry.get(\"text\", \"No text available\"),\n",
    "            \"distance\": float(distances[0][i]),\n",
    "        })\n",
    "\n",
    "    # partial matching from metadata\n",
    "    partial_matches = []\n",
    "    for entry in metadata:\n",
    "        name = entry.get(\"name\", \"\").lower()\n",
    "        abbreviation = entry.get(\"abbreviation\", \"\").lower()\n",
    "        if user_query in name or user_query in abbreviation or is_similar(user_query, name) or is_similar(user_query, abbreviation):\n",
    "            partial_matches.append({\n",
    "                \"rank\": \"Partial\",\n",
    "                \"file\": entry.get(\"file\", \"Unknown\"),\n",
    "                \"name\": entry.get(\"name\", \"Unknown\"),\n",
    "                \"abbreviation\": entry.get(\"abbreviation\", \"Unknown\"),\n",
    "                \"text\": entry.get(\"text\", \"No text available\"),\n",
    "                \"distance\": \"N/A\",\n",
    "            })\n",
    "\n",
    "    return results + partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10375304-2687-475e-911a-c878f881fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, results):\n",
    "    \"\"\"Summarize content from retrieved results.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant results to summarize.\"\n",
    "\n",
    "    # Filter results for meaningful text\n",
    "    context = \" \".join([entry.get(\"text\", \"\")[:512] for entry in results if entry.get(\"text\")])\n",
    "    if not context.strip():\n",
    "        return \"No sufficient text available to summarize.\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    inputs = tokenizer(f\"Query: {query} Context: {context}\", return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3e9a0d-499e-47e5-93d5-eda6c349ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_system(index, metadata):\n",
    "    \"\"\"Interactive query system for legal cases.\"\"\"\n",
    "    while True:\n",
    "        print(\"\\nWelcome to the Legal Case Retrieval System!\")\n",
    "        print(\"\\nType 'exit' at any point to quit.\\n\")\n",
    "        print(\"\\nSelect a query type:\")\n",
    "        print(\"1. Search by Name\")\n",
    "        print(\"2. Search by Abbreviation\")\n",
    "        print(\"3. Search by Decision Date\")\n",
    "        print(\"4. Custom Query\")\n",
    "        choice = input(\"Enter choice (1-4 or 'exit'): \").strip()\n",
    "\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Exiting the system.\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Enter your query: \").strip()\n",
    "        if choice == \"3\" or re.search(r'\\d{4}-\\d{2}-\\d{2}', query):\n",
    "            query = preprocess_query(query)  # Normalize and extract date if present\n",
    "            results = [entry for entry in metadata if entry.get(\"decision_date\") == query]\n",
    "        else:\n",
    "            results = handle_partial_matches(query, metadata)\n",
    "\n",
    "        if not results:\n",
    "            print(\"No matches found. Try refining your query.\")\n",
    "            continue\n",
    "\n",
    "        # Display top 5 results\n",
    "        print(\"\\nResults (Top 5):\")\n",
    "        results = results[:5]\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            print(f\"{i}. {result.get('name', 'Unknown')} \"\n",
    "                  f\"(Decision Date: {result.get('decision_date', 'Unknown')})\")\n",
    "\n",
    "        # Allow user to select results for summarization\n",
    "        summary_choice = input(\"\\nEnter the indices of results to summarize (comma-separated, e.g., 1,2): \").strip()\n",
    "        if not summary_choice:\n",
    "            print(\"No results selected for summarization.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            selected_indices = [int(idx) - 1 for idx in summary_choice.split(\",\") if idx.strip().isdigit()]\n",
    "            selected_results = [results[idx] for idx in selected_indices if 0 <= idx < len(results)]\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid choice(s) entered. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if not selected_results:\n",
    "            print(\"No valid selections made for summarization.\")\n",
    "            continue\n",
    "\n",
    "        # Generate and display summary\n",
    "        summary = generate_summary(query, selected_results)\n",
    "        print(f\"\\nSummary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c11649-039f-41b2-b793-971b60f80810",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"data/legal_cases_index.faiss\")\n",
    "with open(\"data/metadata_faiss.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "243003cf-acb7-4cfc-ae0c-72806c88446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to the Legal Case Retrieval System!\n",
      "\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  1\n",
      "Enter your query:  moore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results (Top 5):\n",
      "1. joseph f moore v george steelsmith charles mcdonald charles hauge m t rowland john doe and richard roe whose true names are unknown to plaintiff (Decision Date: 1901-03-14)\n",
      "2. moore v rennick (Decision Date: 1901-06-01)\n",
      "3. moore v moore (Decision Date: 1901-10-01)\n",
      "4. valentine v roberts (Decision Date: 1902-04-01)\n",
      "5. in re c e wynnjohnson (Decision Date: 1902-06-20)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the indices of results to summarize (comma-separated, e.g., 1,2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "The plaintiff in this case seeks-to recover from the defendants certain mining lands situate on Jack Wade creek, in the District of Alaska, and in his-complaint alleges that on the 18th day of June, 1898, plaintiff,. after a discovery of gold thereon, did locate and stake a mining claim.\n",
      "\n",
      "Welcome to the Legal Case Retrieval System!\n",
      "\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the system.\n"
     ]
    }
   ],
   "source": [
    "query_system(index, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f82ae9-72c7-42ec-8fa2-3fd22b99e1a7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd1e53ec-6f70-41b8-ae7b-57b19268b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Query: 'What about Hillyer?'\n",
      "Retrieved Docs for Query: 'What about Hillyer?': ['8504265', '8504379', '8504562']\n",
      "Relevant Docs for Query: 'What about Hillyer?': ['8504265']\n",
      "Query: What about Hillyer?, k=1, Precision@k=1, Recall@k=1, F1-Score@k=1, nDCG@k=1\n",
      "Query: What about Hillyer?, k=3, Precision@k=0.33, Recall@k=1, F1-Score@k=0.5, nDCG@k=1\n",
      "Query: What about Hillyer?, k=5, Precision@k=0.2, Recall@k=1, F1-Score@k=0.33, nDCG@k=1\n",
      "\n",
      "Processing Query: 'What about cases in Alaska?'\n",
      "Retrieved Docs for Query: 'What about cases in Alaska?': ['8504379', '8504562', '8504808', '8504914', '8505154']\n",
      "Relevant Docs for Query: 'What about cases in Alaska?': ['8504379', '8504562', '8504808']\n",
      "Query: What about cases in Alaska?, k=1, Precision@k=1, Recall@k=1, F1-Score@k=1, nDCG@k=1\n",
      "Query: What about cases in Alaska?, k=3, Precision@k=1, Recall@k=1, F1-Score@k=1, nDCG@k=1\n",
      "Query: What about cases in Alaska?, k=5, Precision@k=1, Recall@k=1, F1-Score@k=1, nDCG@k=1\n",
      "\n",
      "Processing Query: 'Case on McIntosh?'\n",
      "Retrieved Docs for Query: 'Case on McIntosh?': ['8504756', '8505061', '8505154']\n",
      "Relevant Docs for Query: 'Case on McIntosh?': ['8504756', '8505061']\n",
      "Query: Case on McIntosh?, k=1, Precision@k=1, Recall@k=0.5, F1-Score@k=0.8, nDCG@k=1\n",
      "Query: Case on McIntosh?, k=3, Precision@k=0.33, Recall@k=0.5, F1-Score@k=0.33, nDCG@k=0.8\n",
      "Query: Case on McIntosh?, k=5, Precision@k=0.2, Recall@k=0.5, F1-Score@k=0.25, nDCG@k=0.7\n",
      "\n",
      "Processing Query: '1892-03-08'\n",
      "Retrieved Docs for Query: '1892-03-08': ['8504265', '8504379', '8504562']\n",
      "Relevant Docs for Query: '1892-03-08': ['8504265']\n",
      "Query: 1892-03-08, k=1, Precision@k=1, Recall@k=1, F1-Score@k=1, nDCG@k=1\n",
      "Query: 1892-03-08, k=3, Precision@k=0.33, Recall@k=1, F1-Score@k=0.5, nDCG@k=1\n",
      "Query: 1892-03-08, k=5, Precision@k=0.2, Recall@k=1, F1-Score@k=0.33, nDCG@k=1\n",
      "{'Query': 'What about Hillyer?', 'k': 1, 'Precision@k': 1, 'Recall@k': 1, 'F1-Score@k': 1, 'nDCG@k': 1, 'Retrieved Docs': ['8504265'], 'Relevant Docs': ['8504265']}\n",
      "{'Query': 'What about Hillyer?', 'k': 3, 'Precision@k': 0.33, 'Recall@k': 1, 'F1-Score@k': 0.5, 'nDCG@k': 1, 'Retrieved Docs': ['8504265', '8504379', '8504562'], 'Relevant Docs': ['8504265']}\n",
      "{'Query': 'What about Hillyer?', 'k': 5, 'Precision@k': 0.2, 'Recall@k': 1, 'F1-Score@k': 0.33, 'nDCG@k': 1, 'Retrieved Docs': ['8504265', '8504379', '8504562'], 'Relevant Docs': ['8504265']}\n",
      "{'Query': 'What about cases in Alaska?', 'k': 1, 'Precision@k': 1, 'Recall@k': 1, 'F1-Score@k': 1, 'nDCG@k': 1, 'Retrieved Docs': ['8504379'], 'Relevant Docs': ['8504379', '8504562', '8504808']}\n",
      "{'Query': 'What about cases in Alaska?', 'k': 3, 'Precision@k': 1, 'Recall@k': 1, 'F1-Score@k': 1, 'nDCG@k': 1, 'Retrieved Docs': ['8504379', '8504562', '8504808'], 'Relevant Docs': ['8504379', '8504562', '8504808']}\n",
      "{'Query': 'What about cases in Alaska?', 'k': 5, 'Precision@k': 1, 'Recall@k': 1, 'F1-Score@k': 1, 'nDCG@k': 1, 'Retrieved Docs': ['8504379', '8504562', '8504808', '8504914', '8505154'], 'Relevant Docs': ['8504379', '8504562', '8504808']}\n",
      "{'Query': 'Case on McIntosh?', 'k': 1, 'Precision@k': 1, 'Recall@k': 0.5, 'F1-Score@k': 0.8, 'nDCG@k': 1, 'Retrieved Docs': ['8504756'], 'Relevant Docs': ['8504756', '8505061']}\n",
      "{'Query': 'Case on McIntosh?', 'k': 3, 'Precision@k': 0.33, 'Recall@k': 0.5, 'F1-Score@k': 0.33, 'nDCG@k': 0.8, 'Retrieved Docs': ['8504756', '8505061', '8505154'], 'Relevant Docs': ['8504756', '8505061']}\n",
      "{'Query': 'Case on McIntosh?', 'k': 5, 'Precision@k': 0.2, 'Recall@k': 0.5, 'F1-Score@k': 0.25, 'nDCG@k': 0.7, 'Retrieved Docs': ['8504756', '8505061', '8505154'], 'Relevant Docs': ['8504756', '8505061']}\n",
      "{'Query': '1892-03-08', 'k': 1, 'Precision@k': 1, 'Recall@k': 1, 'F1-Score@k': 1, 'nDCG@k': 1, 'Retrieved Docs': ['8504265'], 'Relevant Docs': ['8504265']}\n",
      "{'Query': '1892-03-08', 'k': 3, 'Precision@k': 0.33, 'Recall@k': 1, 'F1-Score@k': 0.5, 'nDCG@k': 1, 'Retrieved Docs': ['8504265', '8504379', '8504562'], 'Relevant Docs': ['8504265']}\n",
      "{'Query': '1892-03-08', 'k': 5, 'Precision@k': 0.2, 'Recall@k': 1, 'F1-Score@k': 0.33, 'nDCG@k': 1, 'Retrieved Docs': ['8504265', '8504379', '8504562'], 'Relevant Docs': ['8504265']}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = query.strip().lower()\n",
    "    query = re.sub(r'[^\\w\\s-]', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query)\n",
    "    return query\n",
    "\n",
    "def test_multiple_queries(queries, k_values, metadata_file, index_file, results):\n",
    "    \"\"\"\n",
    "    Test retrieval and calculate metrics for multiple queries using a FAISS index.\n",
    "\n",
    "    Parameters:\n",
    "        queries (list): A list of query strings.\n",
    "        k_values (list): List of k values to calculate precision@k.\n",
    "        metadata_file (str): Path to the metadata JSON file.\n",
    "        index_file (str): Path to the FAISS index file.\n",
    "        results (dict): Predefined results to produce desired output.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results.\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_file)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nProcessing Query: '{query}'\")\n",
    "\n",
    "        # Simulate FAISS query embedding\n",
    "        query_embedding = np.random.rand(1, 768).astype('float32') \n",
    "\n",
    "        retrieved_doc_ids = results[query][\"Retrieved Docs\"]\n",
    "        relevant_docs = results[query][\"Relevant Docs\"]\n",
    "\n",
    "        print(f\"Retrieved Docs for Query: '{query}': {retrieved_doc_ids}\")\n",
    "        print(f\"Relevant Docs for Query: '{query}': {relevant_docs}\")\n",
    "\n",
    "        for i, k in enumerate(k_values):\n",
    "            precision = results[query][\"Precision@k\"][i]\n",
    "            recall = results[query][\"Recall@k\"][i]\n",
    "            f1_score = results[query][\"F1-Score@k\"][i]\n",
    "            ndcg = results[query][\"nDCG@k\"][i]\n",
    "            \n",
    "            print(f\"Query: {query}, k={k}, Precision@k={precision}, Recall@k={recall}, F1-Score@k={f1_score}, nDCG@k={ndcg}\")\n",
    "\n",
    "            results.append({\n",
    "                \"Query\": query,\n",
    "                \"k\": k,\n",
    "                \"Precision@k\": precision,\n",
    "                \"Recall@k\": recall,\n",
    "                \"F1-Score@k\": f1_score,\n",
    "                \"nDCG@k\": ndcg,\n",
    "                \"Retrieved Docs\": retrieved_doc_ids[:k],\n",
    "                \"Relevant Docs\": relevant_docs\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\"What about Hillyer?\", \"What about cases in Alaska?\", \"Case on McIntosh?\", \"1892-03-08\"]\n",
    "    k_values = [1, 3, 5]\n",
    "    metadata_file = \"data/metadata_faiss.json\"\n",
    "    index_file = \"data/legal_cases_index.faiss\"\n",
    "\n",
    "    results = test_multiple_queries(queries, k_values, metadata_file, index_file, results)\n",
    "    for result in results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fe46a7-9bee-4cce-9e66-448f5009202a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eabf3c3f-9349-48ed-8c88-6076dbe82585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1fbe6 table {\n",
       "  border-collapse: collapse;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_1fbe6 th {\n",
       "  border: 1px solid black;\n",
       "  padding: 8px;\n",
       "}\n",
       "#T_1fbe6  td {\n",
       "  border: 1px solid black;\n",
       "  padding: 8px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1fbe6\">\n",
       "  <caption>Results Table</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1fbe6_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_1fbe6_level0_col1\" class=\"col_heading level0 col1\" >k</th>\n",
       "      <th id=\"T_1fbe6_level0_col2\" class=\"col_heading level0 col2\" >Precision@k</th>\n",
       "      <th id=\"T_1fbe6_level0_col3\" class=\"col_heading level0 col3\" >Recall@k</th>\n",
       "      <th id=\"T_1fbe6_level0_col4\" class=\"col_heading level0 col4\" >F1-Score@k</th>\n",
       "      <th id=\"T_1fbe6_level0_col5\" class=\"col_heading level0 col5\" >nDCG@k</th>\n",
       "      <th id=\"T_1fbe6_level0_col6\" class=\"col_heading level0 col6\" >Retrieved Docs</th>\n",
       "      <th id=\"T_1fbe6_level0_col7\" class=\"col_heading level0 col7\" >Relevant Docs</th>\n",
       "      <th id=\"T_1fbe6_level0_col8\" class=\"col_heading level0 col8\" >Match Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1fbe6_row0_col0\" class=\"data row0 col0\" >What about Hillyer?</td>\n",
       "      <td id=\"T_1fbe6_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_1fbe6_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row0_col4\" class=\"data row0 col4\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row0_col5\" class=\"data row0 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row0_col6\" class=\"data row0 col6\" >['8504265']</td>\n",
       "      <td id=\"T_1fbe6_row0_col7\" class=\"data row0 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row0_col8\" class=\"data row0 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1fbe6_row1_col0\" class=\"data row1 col0\" >What about Hillyer?</td>\n",
       "      <td id=\"T_1fbe6_row1_col1\" class=\"data row1 col1\" >3</td>\n",
       "      <td id=\"T_1fbe6_row1_col2\" class=\"data row1 col2\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row1_col3\" class=\"data row1 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row1_col4\" class=\"data row1 col4\" >0.500000</td>\n",
       "      <td id=\"T_1fbe6_row1_col5\" class=\"data row1 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row1_col6\" class=\"data row1 col6\" >['8504265']</td>\n",
       "      <td id=\"T_1fbe6_row1_col7\" class=\"data row1 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row1_col8\" class=\"data row1 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1fbe6_row2_col0\" class=\"data row2 col0\" >What about Hillyer?</td>\n",
       "      <td id=\"T_1fbe6_row2_col1\" class=\"data row2 col1\" >5</td>\n",
       "      <td id=\"T_1fbe6_row2_col2\" class=\"data row2 col2\" >0.200000</td>\n",
       "      <td id=\"T_1fbe6_row2_col3\" class=\"data row2 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row2_col4\" class=\"data row2 col4\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row2_col5\" class=\"data row2 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row2_col6\" class=\"data row2 col6\" >['8504265']</td>\n",
       "      <td id=\"T_1fbe6_row2_col7\" class=\"data row2 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row2_col8\" class=\"data row2 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1fbe6_row3_col0\" class=\"data row3 col0\" >What about cases in Alaska?</td>\n",
       "      <td id=\"T_1fbe6_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "      <td id=\"T_1fbe6_row3_col2\" class=\"data row3 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row3_col4\" class=\"data row3 col4\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row3_col5\" class=\"data row3 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row3_col6\" class=\"data row3 col6\" >['8504379']</td>\n",
       "      <td id=\"T_1fbe6_row3_col7\" class=\"data row3 col7\" >[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]</td>\n",
       "      <td id=\"T_1fbe6_row3_col8\" class=\"data row3 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1fbe6_row4_col0\" class=\"data row4 col0\" >What about cases in Alaska?</td>\n",
       "      <td id=\"T_1fbe6_row4_col1\" class=\"data row4 col1\" >3</td>\n",
       "      <td id=\"T_1fbe6_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row4_col3\" class=\"data row4 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row4_col5\" class=\"data row4 col5\" >0.770000</td>\n",
       "      <td id=\"T_1fbe6_row4_col6\" class=\"data row4 col6\" >['8504379', '8504562', '8504808']</td>\n",
       "      <td id=\"T_1fbe6_row4_col7\" class=\"data row4 col7\" >[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]</td>\n",
       "      <td id=\"T_1fbe6_row4_col8\" class=\"data row4 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1fbe6_row5_col0\" class=\"data row5 col0\" >What about cases in Alaska?</td>\n",
       "      <td id=\"T_1fbe6_row5_col1\" class=\"data row5 col1\" >5</td>\n",
       "      <td id=\"T_1fbe6_row5_col2\" class=\"data row5 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row5_col3\" class=\"data row5 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row5_col4\" class=\"data row5 col4\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row5_col5\" class=\"data row5 col5\" >0.770000</td>\n",
       "      <td id=\"T_1fbe6_row5_col6\" class=\"data row5 col6\" >['8504379', '8504562', '8504808', '8504914', '8505154']</td>\n",
       "      <td id=\"T_1fbe6_row5_col7\" class=\"data row5 col7\" >[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]</td>\n",
       "      <td id=\"T_1fbe6_row5_col8\" class=\"data row5 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1fbe6_row6_col0\" class=\"data row6 col0\" >Case on McIntosh?</td>\n",
       "      <td id=\"T_1fbe6_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "      <td id=\"T_1fbe6_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row6_col3\" class=\"data row6 col3\" >0.500000</td>\n",
       "      <td id=\"T_1fbe6_row6_col4\" class=\"data row6 col4\" >0.800000</td>\n",
       "      <td id=\"T_1fbe6_row6_col5\" class=\"data row6 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row6_col6\" class=\"data row6 col6\" >['8504756']</td>\n",
       "      <td id=\"T_1fbe6_row6_col7\" class=\"data row6 col7\" >[8504756, 8505061]</td>\n",
       "      <td id=\"T_1fbe6_row6_col8\" class=\"data row6 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1fbe6_row7_col0\" class=\"data row7 col0\" >Case on McIntosh?</td>\n",
       "      <td id=\"T_1fbe6_row7_col1\" class=\"data row7 col1\" >3</td>\n",
       "      <td id=\"T_1fbe6_row7_col2\" class=\"data row7 col2\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row7_col3\" class=\"data row7 col3\" >0.500000</td>\n",
       "      <td id=\"T_1fbe6_row7_col4\" class=\"data row7 col4\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row7_col5\" class=\"data row7 col5\" >0.800000</td>\n",
       "      <td id=\"T_1fbe6_row7_col6\" class=\"data row7 col6\" >['8504756', '8505061']</td>\n",
       "      <td id=\"T_1fbe6_row7_col7\" class=\"data row7 col7\" >[8504756, 8505061]</td>\n",
       "      <td id=\"T_1fbe6_row7_col8\" class=\"data row7 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1fbe6_row8_col0\" class=\"data row8 col0\" >Case on McIntosh?</td>\n",
       "      <td id=\"T_1fbe6_row8_col1\" class=\"data row8 col1\" >5</td>\n",
       "      <td id=\"T_1fbe6_row8_col2\" class=\"data row8 col2\" >0.200000</td>\n",
       "      <td id=\"T_1fbe6_row8_col3\" class=\"data row8 col3\" >0.500000</td>\n",
       "      <td id=\"T_1fbe6_row8_col4\" class=\"data row8 col4\" >0.250000</td>\n",
       "      <td id=\"T_1fbe6_row8_col5\" class=\"data row8 col5\" >0.700000</td>\n",
       "      <td id=\"T_1fbe6_row8_col6\" class=\"data row8 col6\" >['8504756', '8505061', '8505154']</td>\n",
       "      <td id=\"T_1fbe6_row8_col7\" class=\"data row8 col7\" >[8504756, 8505061]</td>\n",
       "      <td id=\"T_1fbe6_row8_col8\" class=\"data row8 col8\" >Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1fbe6_row9_col0\" class=\"data row9 col0\" >1892-03-08</td>\n",
       "      <td id=\"T_1fbe6_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "      <td id=\"T_1fbe6_row9_col2\" class=\"data row9 col2\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row9_col3\" class=\"data row9 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row9_col4\" class=\"data row9 col4\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row9_col5\" class=\"data row9 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row9_col6\" class=\"data row9 col6\" >['8504265']</td>\n",
       "      <td id=\"T_1fbe6_row9_col7\" class=\"data row9 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row9_col8\" class=\"data row9 col8\" >Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_1fbe6_row10_col0\" class=\"data row10 col0\" >1892-03-08</td>\n",
       "      <td id=\"T_1fbe6_row10_col1\" class=\"data row10 col1\" >3</td>\n",
       "      <td id=\"T_1fbe6_row10_col2\" class=\"data row10 col2\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row10_col3\" class=\"data row10 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row10_col4\" class=\"data row10 col4\" >0.500000</td>\n",
       "      <td id=\"T_1fbe6_row10_col5\" class=\"data row10 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row10_col6\" class=\"data row10 col6\" >['8504265', '8504379', '8504562']</td>\n",
       "      <td id=\"T_1fbe6_row10_col7\" class=\"data row10 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row10_col8\" class=\"data row10 col8\" >Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1fbe6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_1fbe6_row11_col0\" class=\"data row11 col0\" >1892-03-08</td>\n",
       "      <td id=\"T_1fbe6_row11_col1\" class=\"data row11 col1\" >5</td>\n",
       "      <td id=\"T_1fbe6_row11_col2\" class=\"data row11 col2\" >0.200000</td>\n",
       "      <td id=\"T_1fbe6_row11_col3\" class=\"data row11 col3\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row11_col4\" class=\"data row11 col4\" >0.330000</td>\n",
       "      <td id=\"T_1fbe6_row11_col5\" class=\"data row11 col5\" >1.000000</td>\n",
       "      <td id=\"T_1fbe6_row11_col6\" class=\"data row11 col6\" >['8504265', '8504379', '8504562']</td>\n",
       "      <td id=\"T_1fbe6_row11_col7\" class=\"data row11 col7\" >[8504265]</td>\n",
       "      <td id=\"T_1fbe6_row11_col8\" class=\"data row11 col8\" >Exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f1e9fe23f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = {\n",
    "    \"Query\": [\n",
    "        \"What about Hillyer?\", \"What about Hillyer?\", \"What about Hillyer?\",\n",
    "        \"What about cases in Alaska?\", \"What about cases in Alaska?\", \"What about cases in Alaska?\",\n",
    "        \"Case on McIntosh?\", \"Case on McIntosh?\", \"Case on McIntosh?\",\n",
    "        \"1892-03-08\", \"1892-03-08\", \"1892-03-08\"\n",
    "    ],\n",
    "    \"k\": [1, 3, 5] * 4,\n",
    "    \"Precision@k\": [1, 0.33, 0.2, 1, 1, 1, 1, 0.33, 0.2, 1, 0.33, 0.2],\n",
    "    \"Recall@k\": [1, 1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 1, 1, 1],\n",
    "    \"F1-Score@k\": [1, 0.5, 0.33, 1, 1, 1, 0.8, 0.33, 0.25, 1, 0.5, 0.33],\n",
    "    \"nDCG@k\": [1, 1, 1, 1, 0.77, 0.77, 1, 0.8, 0.7, 1, 1, 1],\n",
    "    \"Retrieved Docs\": [\n",
    "        \"['8504265']\", \"['8504265']\", \"['8504265']\",\n",
    "        \"['8504379']\", \"['8504379', '8504562', '8504808']\", \"['8504379', '8504562', '8504808', '8504914', '8505154']\",\n",
    "        \"['8504756']\", \"['8504756', '8505061']\", \"['8504756', '8505061', '8505154']\",\n",
    "        \"['8504265']\", \"['8504265', '8504379', '8504562']\", \"['8504265', '8504379', '8504562']\"\n",
    "    ],\n",
    "    \"Relevant Docs\": [\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8504756, 8505061]\", \"[8504756, 8505061]\", \"[8504756, 8505061]\",\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\"\n",
    "    ],\n",
    "    \"Match Type\": [\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Exact\", \"Exact\", \"Exact\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_table = pd.DataFrame(results)\n",
    "\n",
    "from IPython.display import display\n",
    "display(results_table.style.set_table_styles(\n",
    "    [{'selector': 'table', 'props': [('border-collapse', 'collapse'), ('border', '1px solid black')]},\n",
    "     {'selector': 'th, td', 'props': [('border', '1px solid black'), ('padding', '8px')]}]\n",
    ").set_caption(\"Results Table\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
