{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f90a304-074b-4274-90e8-e70eccacb113",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Saving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee27799-49c2-46e5-ae93-0f38b7de23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1494b2d-1ae8-4b33-b138-4ccf37cc4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81995c7c-f547-4861-8bc9-6b306d6a79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date(date):\n",
    "    \"\"\"Standardize date to YYYY-MM-DD format.\"\"\"\n",
    "    try:\n",
    "        if len(date) == 4:  # Year only\n",
    "            return pd.to_datetime(f\"{date}-01-01\").strftime(\"%Y-%m-%d\")\n",
    "        elif len(date) == 7:  # Year and month\n",
    "            return pd.to_datetime(f\"{date}-01\").strftime(\"%Y-%m-%d\")\n",
    "        else:  # Full date\n",
    "            return pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3065f4b-cdf6-47d7-9b30-109ac6ea04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_metadata(json_dir, metadata_file):\n",
    "    \"\"\"Preprocess raw JSON files and save metadata for FAISS.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(json_dir, file_name), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                row = {\n",
    "                    \"file\": file_name,\n",
    "                    \"name\": preprocess_text(data.get(\"name\", \"\")),\n",
    "                    \"abbreviation\": preprocess_text(data.get(\"name_abbreviation\", \"\")),\n",
    "                    \"decision_date\": standardize_date(data.get(\"decision_date\", \"\")),\n",
    "                    \"text\": \" \".join(opinion.get(\"text\", \"\") for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Save processed metadata\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49001734-d61e-41cd-8d5d-ea36fb9ec02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to data/metadata_faiss.json\n"
     ]
    }
   ],
   "source": [
    "json_dir = \"json/\"  # Folder containing raw JSON files\n",
    "metadata_file = \"data/metadata_faiss.json\"  # Metadata file for FAISS\n",
    "preprocess_and_save_metadata(json_dir, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a791f-0551-49c0-b9c3-9c24f7c204eb",
   "metadata": {},
   "source": [
    "## 2. Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dc5685-be3b-41cf-9cba-126f1475a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177b00bc-1704-4642-a9a3-14d693829ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Generate embeddings for text.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9db511-1399-4f6e-a92b-1ff0090f2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(metadata_file, index_file):\n",
    "    \"\"\"Create a FAISS index from metadata.\"\"\"\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Generate embeddings for each text\n",
    "    embeddings = [embed_text(entry[\"text\"]) for entry in metadata]\n",
    "    embeddings = np.vstack(embeddings)  # Combine embeddings into a matrix\n",
    "\n",
    "    # Create and save FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_file)\n",
    "    print(f\"FAISS index saved to {index_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8a30d4-57e8-45fc-9472-5a9b756bc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-10 01:20:02.148596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to data/legal_cases_index.faiss\n"
     ]
    }
   ],
   "source": [
    "create_faiss_index(\"data/metadata_faiss.json\", \"data/legal_cases_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a94be-7c6a-4564-91d4-5723210bb26e",
   "metadata": {},
   "source": [
    "## 3. Interactive Query System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af2942d4-58ab-40d4-937c-3c9d0af68323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8efbf890-f558-4c04-b36e-c39564de96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(a, b, threshold=0.8):\n",
    "    \"\"\"Check if two strings are similar using SequenceMatcher.\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio() > threshold\n",
    "\n",
    "def handle_partial_matches(query, metadata):\n",
    "    \"\"\"Retrieve and rank partial matches.\"\"\"\n",
    "    query_normalized = preprocess_query(query)\n",
    "    results = []\n",
    "\n",
    "    for entry in metadata:\n",
    "        name = preprocess_query(entry.get(\"name\", \"\"))\n",
    "        abbreviation = preprocess_query(entry.get(\"abbreviation\", \"\"))\n",
    "        \n",
    "        if query_normalized in name or query_normalized in abbreviation or is_similar(query_normalized, name) or is_similar(query_normalized, abbreviation):\n",
    "            results.append({\n",
    "                \"name\": entry.get(\"name\", \"Unknown\"),\n",
    "                \"abbreviation\": entry.get(\"abbreviation\", \"Unknown\"),\n",
    "                \"decision_date\": entry.get(\"decision_date\", \"Unknown\"),\n",
    "                \"jurisdiction\": entry.get(\"jurisdiction\", \"Unknown\"),\n",
    "                \"text\": entry.get(\"cleaned_text\", \"No text available\")\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23460511-05a6-469a-9f97-8423c18a579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = query.strip().lower()\n",
    "    query = re.sub(r'[^\\w\\s-]', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = [\n",
    "        'what about', 'can you', 'please', 'show me', 'find', \n",
    "        'search for', 'give me', 'how about', 'tell me about'\n",
    "    ]\n",
    "    for filler in filler_words:\n",
    "        query = re.sub(r'\\b' + re.escape(filler) + r'\\b', '', query)\n",
    "\n",
    "    return query.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9059bd-3fff-4560-a525-c00a5f8afefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(user_query, index, metadata, k=5):\n",
    "    \"\"\"Query FAISS index and return top results with partial matching.\"\"\"\n",
    "    query_embedding = embed_text(user_query)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = []\n",
    "\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        metadata_entry = metadata[idx]\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"file\": metadata_entry.get(\"file\", \"Unknown\"),\n",
    "            \"name\": metadata_entry.get(\"name\", \"Unknown\"),\n",
    "            \"abbreviation\": metadata_entry.get(\"abbreviation\", \"Unknown\"),\n",
    "            \"text\": metadata_entry.get(\"text\", \"No text available\"),\n",
    "            \"distance\": float(distances[0][i]),\n",
    "        })\n",
    "\n",
    "    # partial matching from metadata\n",
    "    partial_matches = []\n",
    "    for entry in metadata:\n",
    "        name = entry.get(\"name\", \"\").lower()\n",
    "        abbreviation = entry.get(\"abbreviation\", \"\").lower()\n",
    "        if user_query in name or user_query in abbreviation or is_similar(user_query, name) or is_similar(user_query, abbreviation):\n",
    "            partial_matches.append({\n",
    "                \"rank\": \"Partial\",\n",
    "                \"file\": entry.get(\"file\", \"Unknown\"),\n",
    "                \"name\": entry.get(\"name\", \"Unknown\"),\n",
    "                \"abbreviation\": entry.get(\"abbreviation\", \"Unknown\"),\n",
    "                \"text\": entry.get(\"text\", \"No text available\"),\n",
    "                \"distance\": \"N/A\",\n",
    "            })\n",
    "\n",
    "    return results + partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10375304-2687-475e-911a-c878f881fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, results):\n",
    "    \"\"\"Summarize content from retrieved results.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant results to summarize.\"\n",
    "    \n",
    "    context = \" \".join([doc.get(\"text\", \"\")[:1024] for doc in results])  # Limit length for summarization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    inputs = tokenizer(f\"Query: {query} Context: {context}\", return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa3e9a0d-499e-47e5-93d5-eda6c349ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_system(index, metadata):\n",
    "    \"\"\"Interactive query system for legal cases.\"\"\"\n",
    "    while True:\n",
    "        print(\"\\nSelect a query type:\")\n",
    "        print(\"1. Search by Name\")\n",
    "        print(\"2. Search by Abbreviation\")\n",
    "        print(\"3. Search by Decision Date\")\n",
    "        print(\"4. Custom Query\")\n",
    "        choice = input(\"Enter choice (1-4 or 'exit'): \").strip()\n",
    "\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Exiting the system.\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Enter your query: \").strip()\n",
    "        if choice == \"3\" and not re.match(r\"\\d{4}-\\d{2}-\\d{2}\", query):\n",
    "            print(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
    "            continue\n",
    "\n",
    "        results = handle_partial_matches(query, metadata)\n",
    "\n",
    "        if not results:\n",
    "            print(\"No matches found. Try refining your query.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\nResults:\")\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            print(f\"{i}. {result['name']} (Decision Date: {result['decision_date']}, Jurisdiction: {result['jurisdiction']})\")\n",
    "\n",
    "        summary_choice = input(\"\\nSummarize results? (yes/no): \").strip().lower()\n",
    "        if summary_choice == \"yes\":\n",
    "            summary = generate_summary(query, results)\n",
    "            print(f\"\\nSummary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4c11649-039f-41b2-b793-971b60f80810",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"data/legal_cases_index.faiss\")\n",
    "with open(\"data/metadata_faiss.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "243003cf-acb7-4cfc-ae0c-72806c88446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  1\n",
      "Enter your query:  pratt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "1. pratt et al v united alaska min co (Decision Date: 1900-10-01, Jurisdiction: Unknown)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize results? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " query: pratt context: No text available. context: Pratt is the name of a famous American baseball player. pratt is also known as the son of legendary baseball player Babe Ruth. Context: Prat is the surname of Babe Ruth, who was born in 1901.\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the system.\n"
     ]
    }
   ],
   "source": [
    "query_system(index, metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
