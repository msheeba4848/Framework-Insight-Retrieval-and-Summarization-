{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f90a304-074b-4274-90e8-e70eccacb113",
   "metadata": {},
   "source": [
    "## 1. Preprocessing and Saving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee27799-49c2-46e5-ae93-0f38b7de23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1494b2d-1ae8-4b33-b138-4ccf37cc4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81995c7c-f547-4861-8bc9-6b306d6a79dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_date(date):\n",
    "    \"\"\"Standardize date to YYYY-MM-DD format.\"\"\"\n",
    "    try:\n",
    "        if len(date) == 4:  # Year only\n",
    "            return pd.to_datetime(f\"{date}-01-01\").strftime(\"%Y-%m-%d\")\n",
    "        elif len(date) == 7:  # Year and month\n",
    "            return pd.to_datetime(f\"{date}-01\").strftime(\"%Y-%m-%d\")\n",
    "        else:  # Full date\n",
    "            return pd.to_datetime(date).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3065f4b-cdf6-47d7-9b30-109ac6ea04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_metadata(json_dir, metadata_file):\n",
    "    \"\"\"Preprocess raw JSON files and save metadata for FAISS.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            with open(os.path.join(json_dir, file_name), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                row = {\n",
    "                    \"file\": file_name,\n",
    "                    \"name\": preprocess_text(data.get(\"name\", \"\")),\n",
    "                    \"abbreviation\": preprocess_text(data.get(\"name_abbreviation\", \"\")),\n",
    "                    \"decision_date\": standardize_date(data.get(\"decision_date\", \"\")),\n",
    "                    \"text\": \" \".join(opinion.get(\"text\", \"\") for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Save processed metadata\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(all_data, f, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49001734-d61e-41cd-8d5d-ea36fb9ec02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to data/metadata_faiss.json\n"
     ]
    }
   ],
   "source": [
    "json_dir = \"json/\"  # Folder containing raw JSON files\n",
    "metadata_file = \"data/metadata_faiss.json\"  # Metadata file for FAISS\n",
    "preprocess_and_save_metadata(json_dir, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a791f-0551-49c0-b9c3-9c24f7c204eb",
   "metadata": {},
   "source": [
    "## 2. Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dc5685-be3b-41cf-9cba-126f1475a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177b00bc-1704-4642-a9a3-14d693829ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Generate embeddings for text.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9db511-1399-4f6e-a92b-1ff0090f2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(metadata_file, index_file):\n",
    "    \"\"\"Create a FAISS index from metadata.\"\"\"\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Generate embeddings for each text\n",
    "    embeddings = [embed_text(entry[\"text\"]) for entry in metadata]\n",
    "    embeddings = np.vstack(embeddings)  # Combine embeddings into a matrix\n",
    "\n",
    "    # Create and save FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_file)\n",
    "    print(f\"FAISS index saved to {index_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8a30d4-57e8-45fc-9472-5a9b756bc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-13 18:01:42.821353: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to data/legal_cases_index.faiss\n"
     ]
    }
   ],
   "source": [
    "create_faiss_index(\"data/metadata_faiss.json\", \"data/legal_cases_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a94be-7c6a-4564-91d4-5723210bb26e",
   "metadata": {},
   "source": [
    "## 3. Interactive Query System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99657d06-f22d-4040-b19f-988cc4e032cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2942d4-58ab-40d4-937c-3c9d0af68323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8efbf890-f558-4c04-b36e-c39564de96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "def is_similar(a, b, threshold=80):\n",
    "    \"\"\"Check if two strings are similar using fuzz.partial_ratio.\"\"\"\n",
    "    return fuzz.partial_ratio(a, b) > threshold\n",
    "\n",
    "def handle_partial_matches(query, metadata, threshold=80):\n",
    "    \"\"\"Retrieve and rank partial matches.\"\"\"\n",
    "    query_normalized = preprocess_query(query)\n",
    "    results = []\n",
    "\n",
    "    for entry in metadata:\n",
    "        name = preprocess_query(entry.get(\"name\", \"\"))\n",
    "        abbreviation = preprocess_query(entry.get(\"abbreviation\", \"\"))\n",
    "        text = preprocess_query(entry.get(\"text\", \"\"))\n",
    "\n",
    "        if fuzz.partial_ratio(query_normalized, name) > threshold or \\\n",
    "           fuzz.partial_ratio(query_normalized, abbreviation) > threshold or \\\n",
    "           fuzz.partial_ratio(query_normalized, text) > threshold:\n",
    "            results.append(entry)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23460511-05a6-469a-9f97-8423c18a579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = query.strip().lower()\n",
    "    query = re.sub(r'[^\\w\\s-]', '', query)\n",
    "    query = re.sub(r'\\s+', ' ', query)\n",
    "    \n",
    "    # Remove filler words\n",
    "    filler_words = [\n",
    "        'what about', 'can you', 'please', 'show me', 'find', \n",
    "        'search for', 'give me', 'how about', 'tell me about',\n",
    "        'what is', 'on', 'the case on', 'case from', 'is there a case'\n",
    "    ]\n",
    "    for filler in filler_words:\n",
    "        query = re.sub(r'\\b' + re.escape(filler) + r'\\b', '', query)\n",
    "\n",
    "    # Extract date if present in the query\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', query)\n",
    "    if match:\n",
    "        return match.group(0)  # Return the date in YYYY-MM-DD format\n",
    "\n",
    "    return query.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e9059bd-3fff-4560-a525-c00a5f8afefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_index(user_query, index, metadata, k=5):\n",
    "    \"\"\"Query FAISS index and return top results with partial matching.\"\"\"\n",
    "    query_embedding = embed_text(user_query)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = []\n",
    "\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        metadata_entry = metadata[idx]\n",
    "        results.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"file\": metadata_entry.get(\"file\", \"Unknown\"),\n",
    "            \"name\": metadata_entry.get(\"name\", \"Unknown\"),\n",
    "            \"abbreviation\": metadata_entry.get(\"abbreviation\", \"Unknown\"),\n",
    "            \"text\": metadata_entry.get(\"text\", \"No text available\"),\n",
    "            \"distance\": float(distances[0][i]),\n",
    "        })\n",
    "\n",
    "    # partial matching from metadata\n",
    "    partial_matches = []\n",
    "    for entry in metadata:\n",
    "        name = entry.get(\"name\", \"\").lower()\n",
    "        abbreviation = entry.get(\"abbreviation\", \"\").lower()\n",
    "        if user_query in name or user_query in abbreviation or is_similar(user_query, name) or is_similar(user_query, abbreviation):\n",
    "            partial_matches.append({\n",
    "                \"rank\": \"Partial\",\n",
    "                \"file\": entry.get(\"file\", \"Unknown\"),\n",
    "                \"name\": entry.get(\"name\", \"Unknown\"),\n",
    "                \"abbreviation\": entry.get(\"abbreviation\", \"Unknown\"),\n",
    "                \"text\": entry.get(\"text\", \"No text available\"),\n",
    "                \"distance\": \"N/A\",\n",
    "            })\n",
    "\n",
    "    return results + partial_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10375304-2687-475e-911a-c878f881fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, results):\n",
    "    \"\"\"Summarize content from retrieved results.\"\"\"\n",
    "    if not results:\n",
    "        return \"No relevant results to summarize.\"\n",
    "\n",
    "    # Filter results for meaningful text\n",
    "    context = \" \".join([entry.get(\"text\", \"\")[:512] for entry in results if entry.get(\"text\")])\n",
    "    if not context.strip():\n",
    "        return \"No sufficient text available to summarize.\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    inputs = tokenizer(f\"Query: {query} Context: {context}\", return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa3e9a0d-499e-47e5-93d5-eda6c349ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_system(index, metadata):\n",
    "    \"\"\"Interactive query system for legal cases.\"\"\"\n",
    "    while True:\n",
    "        print(\"\\nWelcome to the Legal Case Retrieval System!\")\n",
    "        print(\"\\nType 'exit' at any point to quit.\\n\")\n",
    "        print(\"\\nSelect a query type:\")\n",
    "        print(\"1. Search by Name\")\n",
    "        print(\"2. Search by Abbreviation\")\n",
    "        print(\"3. Search by Decision Date\")\n",
    "        print(\"4. Custom Query\")\n",
    "        choice = input(\"Enter choice (1-4 or 'exit'): \").strip()\n",
    "\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Exiting the system.\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Enter your query: \").strip()\n",
    "        if choice == \"3\" or re.search(r'\\d{4}-\\d{2}-\\d{2}', query):\n",
    "            query = preprocess_query(query)  # Normalize and extract date if present\n",
    "            results = [entry for entry in metadata if entry.get(\"decision_date\") == query]\n",
    "        else:\n",
    "            results = handle_partial_matches(query, metadata)\n",
    "\n",
    "        if not results:\n",
    "            print(\"No matches found. Try refining your query.\")\n",
    "            continue\n",
    "\n",
    "        # Display top 5 results\n",
    "        print(\"\\nResults (Top 5):\")\n",
    "        results = results[:5]\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            print(f\"{i}. {result.get('name', 'Unknown')} \"\n",
    "                  f\"(Decision Date: {result.get('decision_date', 'Unknown')})\")\n",
    "\n",
    "        # Allow user to select results for summarization\n",
    "        summary_choice = input(\"\\nEnter the indices of results to summarize (comma-separated, e.g., 1,2): \").strip()\n",
    "        if not summary_choice:\n",
    "            print(\"No results selected for summarization.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            selected_indices = [int(idx) - 1 for idx in summary_choice.split(\",\") if idx.strip().isdigit()]\n",
    "            selected_results = [results[idx] for idx in selected_indices if 0 <= idx < len(results)]\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid choice(s) entered. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if not selected_results:\n",
    "            print(\"No valid selections made for summarization.\")\n",
    "            continue\n",
    "\n",
    "        # Generate and display summary\n",
    "        summary = generate_summary(query, selected_results)\n",
    "        print(f\"\\nSummary:\\n{summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4c11649-039f-41b2-b793-971b60f80810",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"data/legal_cases_index.faiss\")\n",
    "with open(\"data/metadata_faiss.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243003cf-acb7-4cfc-ae0c-72806c88446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to the Legal Case Retrieval System!\n",
      "\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  1\n",
      "Enter your query:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results (Top 5):\n",
      "1. in re jesse scott oliver minor (Decision Date: 1887-10-31)\n",
      "2. united states v the northwest trading co et al (Decision Date: 1888-08-20)\n",
      "3. myers v swineford (Decision Date: 1888-08-24)\n",
      "4. ex parte dubuque (Decision Date: 1888-09-20)\n",
      "5. garside v norval (Decision Date: 1888-10-22)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the indices of results to summarize (comma-separated, e.g., 1,2):  Dunbar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid selections made for summarization.\n",
      "\n",
      "Welcome to the Legal Case Retrieval System!\n",
      "\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-4 or 'exit'):  1\n",
      "Enter your query:  Dunbar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results (Top 5):\n",
      "1. dunbar v de groff (Decision Date: 1888-10-26)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the indices of results to summarize (comma-separated, e.g., 1,2):  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "This is an application by one of the parties, by petition in open court, for an order for leave to take the deposition, out of the District of Alaska, of certain witnesses. Two other motions of a similar character were filed at the same time, and will be disposed of in the same way.\n",
      "\n",
      "Welcome to the Legal Case Retrieval System!\n",
      "\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Custom Query\n"
     ]
    }
   ],
   "source": [
    "query_system(index, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44b134-50d9-4dcd-9f9b-1d28d50dd995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
