{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26c951b-c06c-48c4-b1c9-e9f385558f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1.post100)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.3 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama-index) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.6.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (10.4.0)\n",
      "Collecting pydantic<2.10.0,>=2.7.0 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (8.5.0)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.16.0)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.15.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading jiter-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.3->llama-index) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (0.2.0)\n",
      "Downloading llama_index-0.12.3-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.3-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.3.2-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
      "Downloading llama_parse-0.5.16-py3-none-any.whl (14 kB)\n",
      "Downloading openai-1.57.0-py3-none-any.whl (389 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading jiter-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, pydantic-core, jiter, annotated-types, tiktoken, pydantic, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.17\n",
      "    Uninstalling pydantic-1.10.17:\n",
      "      Successfully uninstalled pydantic-1.10.17\n",
      "Successfully installed annotated-types-0.7.0 dirtyjson-1.0.8 filetype-1.2.0 jiter-0.8.0 llama-cloud-0.1.6 llama-index-0.12.3 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-core-0.12.3 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.3.2 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.16 openai-1.57.0 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 striprtf-0.0.26 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers llama-index scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd20e245-40d1-45f5-b739-4a579a781626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: output_cases.csv\n"
     ]
    }
   ],
   "source": [
    "# json to csv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "def json_to_csv(json_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Converts all JSON files in a directory into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        json_dir (str): Path to the directory containing JSON files.\n",
    "        output_csv (str): Path to save the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate over all JSON files in the directory\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_dir, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Flatten the JSON structure and extract relevant data\n",
    "                row = {\n",
    "                    \"id\": data.get(\"id\"),\n",
    "                    \"name\": data.get(\"name\"),\n",
    "                    \"abbreviation\": data.get(\"name_abbreviation\"),\n",
    "                    \"decision_date\": data.get(\"decision_date\"),\n",
    "                    \"court_name\": data.get(\"court\", {}).get(\"name\"),\n",
    "                    \"jurisdiction_name\": data.get(\"jurisdiction\", {}).get(\"name\"),\n",
    "                    \"word_count\": data.get(\"analysis\", {}).get(\"word_count\"),\n",
    "                    \"char_count\": data.get(\"analysis\", {}).get(\"char_count\"),\n",
    "                    \"ocr_confidence\": data.get(\"analysis\", {}).get(\"ocr_confidence\"),\n",
    "                    \"case_text\": \" \".join([opinion[\"text\"] for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])]),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file saved at: {output_csv}\")\n",
    "\n",
    "# Specify the path to the JSON directory and output CSV file\n",
    "json_dir = \"json/\"\n",
    "output_csv = \"output_cases.csv\"\n",
    "\n",
    "# Convert JSON files to CSV\n",
    "json_to_csv(json_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7fda4fb-63a1-430a-b422-3bf5e150fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0001-01.json\n",
      "{\n",
      "    \"id\": 8503986,\n",
      "    \"name\": \"In re JESSE SCOTT OLIVER, Minor\",\n",
      "    \"name_abbreviation\": \"In re Oliver\",\n",
      "    \"decision_date\": \"1887-10-31\",\n",
      "    \"docket_number\": \"No. 95\",\n",
      "    \"first_page\": \"1\",\n",
      "    \"last_page\": \"4\",\n",
      "    \"citations\": [\n",
      "        {\n",
      "            \"type\": \"official\",\n",
      "            \"cite\": \"1 Alaska 1\"\n",
      "        }\n",
      "    ],\n",
      "    \"court\": {\n",
      "        \"name_abbreviation\": \"Alaska Dist. Ct.\",\n",
      "        \"id\": 23837,\n",
      "        \"name\": \"Alaska District Court\"\n",
      "    },\n",
      "    \"jurisdiction\": {\n",
      "        \"id\": 53,\n",
      "        \"name_long\": \"Alaska\",\n",
      "        \"name\": \"Alaska\"\n",
      "    },\n",
      "    \"cites_to\": [\n",
      "        {\n",
      "            \"cite\": \"6 Am. Dec. 156\",\n",
      "            \"category\": \"reporters:federal\",\n",
      "            \"reporter\": \"Am. Dec.\",\n",
      "            \"opinion_index\": 0\n",
      "        },\n",
      "        {\n",
      "            \"cite\": \"11 Mass. 67\",\n",
      "            \"category\": \"reporters:state\",\n",
      "            \"reporter\": \"Mass.\",\n",
      "            \"case_ids\": [\n",
      "                2053436\n",
      "            ],\n",
      "            \"opinion_index\": 0,\n",
      "            \"case_paths\": [\n",
      "                \"/mass/11/0068-01\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"cite\": \"11 Mass. 63\",\n",
      "            \"category\": \"reporters:state\",\n",
      "            \"reporter\": \"Mass.\",\n",
      "            \"case_ids\": [\n",
      "                2053438\n",
      "            ],\n",
      "            \"opinion_index\": 0,\n",
      "            \"case_paths\": [\n",
      "                \"/mass/11/0065-01\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"cite\": \"21 Stat. 338\",\n",
      "            \"category\": \"laws:leg_session\",\n",
      "            \"reporter\": \"Stat.\",\n",
      "            \"opinion_index\": 0\n",
      "        },\n",
      "        {\n",
      "            \"cite\": \"21 Stat. 3\",\n",
      "            \"category\": \"laws:leg_session\",\n",
      "            \"reporter\": \"Stat.\",\n",
      "            \"opinion_index\": 0\n",
      "        }\n",
      "    ],\n",
      "    \"analysis\": {\n",
      "        \"cardinality\": 416,\n",
      "        \"char_count\": 6764,\n",
      "        \"ocr_confidence\": 0.645,\n",
      "        \"pagerank\": {\n",
      "            \"raw\": 4.03580807328026e-08,\n",
      "            \"percentile\": 0.20520520694449754\n",
      "        },\n",
      "        \"sha256\": \"50f1631f5020f754f3291ca4467f80204cd2fe61308e2afe1caf02925b493229\",\n",
      "        \"simhash\": \"1:d8b7a583688f714b\",\n",
      "        \"word_count\": 1210\n",
      "    },\n",
      "    \"last_updated\": \"2024-02-27T16:57:40.052731+00:00\",\n",
      "    \"provenance\": {\n",
      "        \"date_added\": \"2019-08-29\",\n",
      "        \"source\": \"Harvard\",\n",
      "        \"batch\": \"2018\"\n",
      "    },\n",
      "    \"casebody\": {\n",
      "        \"judges\": [],\n",
      "        \"parties\": [\n",
      "            \"In re JESSE SCOTT OLIVER, Minor.\"\n",
      "        ],\n",
      "        \"opinions\": [\n",
      "            {\n",
      "                \"text\": \"DAWSON, District Judge.\\nPetitioner, by his guardian, ad litem, sets forth that he is unlawfully restrained of his liberty by Lieutenant Commander J. S. Newell, naval officer in charge at this station, and in command of the United States steamer and man-of-war Pinta. He states that he was enlisted into the United States navy before he had attained his majority, and claims that the contract of enlistment is voidable, and that he is entitled to his discharge.\\nThe contract of enlistment in this case, which is similar to all contracts of enlistment in the United States navy, sets forth that petitioner was enlisted on the 8th day of July, 1886, at Mare Island, Cal., to serve as a common seaman for 3 years; that he was at the time of his enlistment 18 years and 7 months old, and that the consent of his parents or guardian had not been obtained. A writ of habeas corpus was issued, made returnable on October 29, 1887, at which time the defendant made return to the suit, embodying substantially the contract of enlistment, and producing the body of Scott Oliver in court. The only evidence in the case is \\u25a0the written contract of enlistment, signed by the petitioner, in which he \\\"states his age to be 18 years and 7 months. The question presented is, can a minor over\\\" 18 years of age bind himself by a contract of enlistment in the United States navy, without the consent of his parents or guardian ?\\nSection 1418, Rev. St. [U. S. Comp. St. 1901, p. 1007], provides that \\u201cboys between the ages of sixteen and eighteen years may be enlisted to serve in the navy until they shall arrive at the age of twent3'-one years; other persons may be enlisted to serve for a period not exceeding five years, unless sooner discharged by direction of the President.\\u201d\\nAgain, section 1419 [U. S. Comp. St. 1901, p. 1007], provides that \\u201cminors between the age of sixteen and eighteen j^ears shall not be enlisted for the naval service without the consent of their parents or guardian.\\u201d The sections quoted were enacted by Congress in. March, 1837, and have been carried forward in the various revisions of the statute since that time.\\nBy an act of Congress approved May 12, 1879 (21 Stat. 3, c. 5), it is provided that no minor under the age of 15 years shall be enlisted in the naval service. Supplement to Rev. St. vol. 1, p. 484 [Q. S. Comp. St. 1901, pp. 1007, 1008]. During the session of Congress of 1881 the former sections in relation to enlistments of minors were again amended as follows:\\n\\u201cThat sections fourteen hundred and eighteen, fourteen hundred and nineteen, fourteen hundred and twenty, as heretofore amended, relating to enlistment of minors in the naval service, be and hereby are amended by striking out the word \\u2018fifteen\\u2019 and inserting in its stead the word \\u2018fourteen.\\u2019 \\u201d\\nThis act was approved on the 23d day of February, 1881 (21 Stat. 338, c. 73). See Rev. St. Supp. p. 595 [U. S. Comp. St. 190j, pp. 1007, 1008]. From these amendments it is quite clear the Congress intended no change as to the right of a minor over the age of 18 years to bind himself by a contract of enlistment. It will be observed that there is- a difference in the matter of legal enlistments in the army and in the navy in regard to age. Section 1117, Rev. St. [U. S. Comp. St. 1901, p. 813], in relation to enlistments in the army, forbids the enlistment of any person under the age of 21 years, without the consent of his parents or guardian.\\nCounsel seems to confound the two provisions, or rather to lose sight of the clear distinction, in the law. The rules of the common law that infants may repudiate their con.tracts after attaining their majority, except where beneficial \\u2014as when made for supplying the necessities of life and the like \\u2014 can have no application to a contract of this nature. It is unlike a contract between private parties. It is an agreement to serve the government, for a period determined by the law, until the minor shall have attained his majority. The government is entitled, by virtue of its sovereignty, to require the services of any or all of its able-bodied citizens, of whatever age, in cases of public exigency. This right, being exercised for the common good, must be regarded as paramount to all individual claims.\\nIt is a part of the law of public policy that neither the rights at common law of the minor contractor nor those of his parent, guardian, or master shall be asserted against the United States, except when expressly recognized by existing statute. The right of the parent, or other person irr loco parentis, to object to the enlistment of a minor in the navy, is clearly limited to cases where the minor is but 18 years of age. Jf he is above that age, and under 21 years of age, he can bind himself by enlisting until he attains his majority, at which time his term expires by operation of law, and at which time the law recognizes his right to choose his vocation and pursue it. This is manifestly the meaning of the law, else adult persons enlisting would not be required to enlist to serve for a period of five years. It has been held on very high authority that enlistments in the navy, though, made without consent of the parent or guardian, are binding, and the minor cannot avoid them. See U. S. v. Bainbridge, 1 Mason, 71, Fed. Cas. No. 14,497; U. S. v. Blakeney, 3 Grat. 405.\\nBut it is otherwise as to enlistments in the army. The distinction is clearly made in the statute, and has been sustained by the courts. See U. S. v. Bainbridge, 1 Mason, 71, Fed. Cas. No. 14,497; Commonwealth v. Harrison, 11 Mass. 63; Com. v. Cushing, 11 Mass. 67, 6 Am. Dec. 156.\\nIn this case it is not disputed that the petitioner signed his name to the contract of enlistment, and represented his age to be 18 years and 7 months. In certain cases, and under certain circumstances, the law of estoppel will apply to minors. That a minor is responsible in damages for his torts and frauds is well settled in the law. If he falsely represents-his age for the purpose of inducing another person to contract with him, he is estopped from afterwards denying it.. See Bigelow on Estoppel, pp. 486, 487.\\nIt follows that the prayer of the petitioner must be denied, and that he be remanded to the custody of Lieutenant Commander Newell and his successors until he is 21 years of age, unless discharged for some other cause; and it is so ordered-\",\n",
      "                \"type\": \"majority\",\n",
      "                \"author\": \"DAWSON, District Judge.\"\n",
      "            }\n",
      "        ],\n",
      "        \"attorneys\": [\n",
      "            \"W. Clark, for petitioner.\",\n",
      "            \"A. McCracken, contra.\"\n",
      "        ],\n",
      "        \"corrections\": \"\",\n",
      "        \"head_matter\": \"In re JESSE SCOTT OLIVER, Minor.\\n(Sitka.\\nOctober 31, 1887.)\\nNo. 95.\\n1. Army and Navy \\u2014 Enlistment\\u2014Habeas Corpus \\u2014 Infants.\\nA minor over eighteen and under twenty-one years of age may enter into a binding contract of enlistment in the navy,' and will not for that reason alone be discharged on habeas corpus.\\nPetition for Habeas Corpus.\\nDenied.\\nW. Clark, for petitioner.\\nA. McCracken, contra.\"\n",
      "    },\n",
      "    \"file_name\": \"0001-01\",\n",
      "    \"first_page_order\": 25,\n",
      "    \"last_page_order\": 28\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_and_inspect_json(folder_path):\n",
    "    \"\"\"Inspect the structure of the first JSON file to debug the issue.\"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File: {file_name}\")\n",
    "                print(json.dumps(data, indent=4))  # Pretty print the JSON structure\n",
    "                break  # Stop after inspecting the first file\n",
    "\n",
    "# Set the folder path to your JSON directory\n",
    "folder_path = \"json/\"\n",
    "load_and_inspect_json(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a530884-b15f-4025-adad-d0070ce391a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated metadata saved to data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Data Loading\n",
    "def load_json_files(folder_path):\n",
    "    \"\"\"Load all JSON files from a given folder into a list of dictionaries.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Preprocessing Functions\n",
    "def preprocess_case_text(text):\n",
    "    \"\"\"Clean and standardize case text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,;:]', '', text)  # Remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_data_with_casebody(data):\n",
    "    \"\"\"Preprocess data by cleaning text and extracting detailed case text.\"\"\"\n",
    "    preprocessed_data = []\n",
    "    for case in data:\n",
    "        # Extract detailed text from 'casebody > opinions > text'\n",
    "        casebody_opinions = case.get(\"casebody\", {}).get(\"opinions\", [])\n",
    "        detailed_text = \" \".join(opinion.get(\"text\", \"\") for opinion in casebody_opinions)\n",
    "\n",
    "        processed_case = {\n",
    "            \"id\": case.get(\"id\"),\n",
    "            \"name\": case.get(\"name\", \"\").strip(),\n",
    "            \"abbreviation\": case.get(\"name_abbreviation\", \"\").strip(),\n",
    "            \"decision_date\": case.get(\"decision_date\", \"\").strip(),\n",
    "            \"jurisdiction\": case.get(\"jurisdiction\", {}).get(\"name\", \"\").strip(),\n",
    "            \"cleaned_text\": preprocess_case_text(detailed_text) if detailed_text else \"No text available\",\n",
    "        }\n",
    "        preprocessed_data.append(processed_case)\n",
    "    return preprocessed_data\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"json/\"  # Path to your folder containing JSON files\n",
    "    data = load_json_files(folder_path)  # Load JSON files\n",
    "    preprocessed_data = preprocess_data_with_casebody(data)  # Preprocess data\n",
    "    \n",
    "    # Save to metadata file for further processing\n",
    "    output_metadata_file = \"data/metadata.json\"\n",
    "    with open(output_metadata_file, \"w\") as f:\n",
    "        json.dump(preprocessed_data, f)\n",
    "\n",
    "    print(f\"Updated metadata saved to {output_metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fdd6abc-8916-48e1-a5da-6528769c754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT Class\n",
    "# ColBERT Class\n",
    "class ColBERT:\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        \"\"\"Generate dense embeddings for a given text.\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "            mask = tokens['attention_mask'].squeeze(0).bool()\n",
    "            return token_embeddings[mask].numpy()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "84fdbbaf-0f49-420b-8d7e-d9d12963dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Preprocessing\n",
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = re.sub(r'\\s+', ' ', query)  # Remove extra whitespace\n",
    "    query = re.sub(r'[^\\w\\s.,;:]', '', query)  # Remove special characters\n",
    "    query = query.strip().lower()\n",
    "\n",
    "    # List of common filler words or phrases to remove\n",
    "    filler_words = [\n",
    "        'what about', 'can you', 'could you', 'please', 'tell me', \n",
    "        'show me', 'find', 'search for', 'give me', 'how about', \n",
    "        'do you know', 'any info on', 'what is', 'can you tell me about', \n",
    "        'let me know', 'is there', 'is it', 'is this', 'i want to know', \n",
    "        'i am looking for', 'can you find', 'what is the status of', \n",
    "        'what do you know', 'have you heard of'\n",
    "    ]\n",
    "    \n",
    "    # Remove common filler words or phrases\n",
    "    for filler in filler_words:\n",
    "        query = re.sub(r'\\b' + re.escape(filler) + r'\\b', '', query)\n",
    "\n",
    "    # Normalize \"v.\" to \"v\" and \"vs\" to \"v\" for consistency\n",
    "    query = query.replace('v.', 'v').replace('vs', 'v')\n",
    "\n",
    "    # Clean up any remaining extra whitespace after removing filler words\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "\n",
    "    return query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30e3ea5d-76b7-492f-8f94-a594153b0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update in `colbert_retrieve` for enhanced exact match\n",
    "def colbert_retrieve(query, embeddings_file, metadata_file, top_k=5):\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    colbert = ColBERT()\n",
    "    query = preprocess_query(query)  # Normalize the query\n",
    "    query_tokens = colbert.tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Generate query embeddings\n",
    "    with torch.no_grad():\n",
    "        query_outputs = colbert.model(**query_tokens)\n",
    "        query_embeddings = query_outputs.last_hidden_state.squeeze(0)\n",
    "        mask = query_tokens['attention_mask'].squeeze(0).bool()\n",
    "        query_embeddings = query_embeddings[mask].numpy()\n",
    "\n",
    "    # Initialize variables to store scores and filtered results\n",
    "    scores = []\n",
    "    filtered_results = []\n",
    "\n",
    "    # Exact matching boost\n",
    "    exact_match_weight = 50\n",
    "\n",
    "    for i, doc in enumerate(metadata):\n",
    "        # Extract case name and abbreviation from metadata\n",
    "        exact_match_score = 0\n",
    "        case_name = doc[\"name\"].lower() if doc.get(\"name\") else \"\"\n",
    "        abbreviation = doc[\"abbreviation\"].lower() if doc.get(\"abbreviation\") else \"\"\n",
    "\n",
    "        # Debug print for document details\n",
    "        #print(f\"Document {i}: Case Name: {case_name} | Abbreviation: {abbreviation}\")\n",
    "\n",
    "        # Normalize case names and abbreviations for exact match\n",
    "        # Normalize case names and abbreviations for exact match\n",
    "        case_name = case_name.replace('v.', 'v').replace('V.', 'v').replace('vs', 'v')\n",
    "        abbreviation = abbreviation.replace('v.', 'v').replace('V.', 'v').replace('vs', 'v')\n",
    "\n",
    "        query = query.replace('v.', 'v').replace('V.', 'v').replace('vs', 'v')\n",
    "\n",
    "\n",
    "        # Exact match check\n",
    "        if query == case_name or query == abbreviation:\n",
    "            exact_match_score = exact_match_weight\n",
    "            print(f\"Exact Match Found: {case_name}\")\n",
    "            filtered_results.append(doc)\n",
    "        \n",
    "        # Document embeddings\n",
    "        doc_embeddings = embeddings[i]\n",
    "        \n",
    "        # Cosine similarity calculation between query and document embeddings\n",
    "        similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        embedding_score = max_similarities.sum()\n",
    "\n",
    "        # Final score: combining exact match score and embedding similarity score\n",
    "        final_score = exact_match_score + embedding_score\n",
    "        scores.append((final_score, i))\n",
    "\n",
    "    # Sort the scores in descending order\n",
    "    scores = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Generate results based on the sorted scores\n",
    "    results = [\n",
    "        {\n",
    "            \"id\": metadata[i][\"id\"],\n",
    "            \"name\": metadata[i][\"name\"],\n",
    "            \"abbreviation\": metadata[i][\"abbreviation\"],\n",
    "            \"decision_date\": metadata[i][\"decision_date\"],\n",
    "            \"jurisdiction\": metadata[i][\"jurisdiction\"],\n",
    "            \"cleaned_text\": metadata[i].get(\"cleaned_text\", \"No text available\"),\n",
    "            \"score\": final_score,\n",
    "        }\n",
    "        for final_score, i in scores[:top_k]\n",
    "    ]\n",
    "\n",
    "    return results, filtered_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c72b269f-ec06-4e93-b13b-5ea253ddf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, retrieved_docs):\n",
    "    \"\"\"Generate a summary using RAG for the most relevant content.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # Combine cleaned_text of retrieved documents\n",
    "    context = \" \".join([doc.get('cleaned_text', '') for doc in retrieved_docs if doc.get('cleaned_text')])\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"No relevant document content found for summarization.\"\n",
    "\n",
    "    # Prepare input for the summarization model\n",
    "    input_text = f\"Query: {query} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaffac5-8c08-46db-a029-4a0b1123f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Search by Jurisdiction\n",
      "5. Custom Legal Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-5):  1\n",
      "Enter case name:  Noon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Cleaned Texts (or Detailed Texts):\n",
      "Document ID: 8503986, Name: In re JESSE SCOTT OLIVER, Minor, Cleaned Text Snippet: DAWSON, District Judge. Petitioner, by his guardian, ad litem, sets forth that he is unlawfully rest...\n",
      "Document ID: 8504008, Name: UNITED STATES v. THE NORTH-WEST TRADING CO. et al., Cleaned Text Snippet: DAWSON, District Judge. On June 6, 1888, the United States District Attorney for the District of Ala...\n",
      "Document ID: 8504024, Name: MYERS v. SWINEFORD, Cleaned Text Snippet: DAAVSON, District Judge. This was an action of assumpsit, brought by plaintiff against the defendant...\n",
      "Document ID: 8504052, Name: Ex parte DUBUQUE, Cleaned Text Snippet: KEATEEY, District Judge. It appears that on the 2ist day of August, 1888, Eouis E. Williams, a Unite...\n",
      "Document ID: 8504080, Name: GARSIDE v. NORVAL, Cleaned Text Snippet: KFATUFY, District Judge. On the 3d of October, 1888, the plaintiff filed a complaint in the office o...\n",
      "\n",
      "Top results:\n",
      "ID: 8503986, Name: In re JESSE SCOTT OLIVER, Minor, Score: 1.5421\n",
      "ID: 8504008, Name: UNITED STATES v. THE NORTH-WEST TRADING CO. et al., Score: 1.5421\n",
      "ID: 8504024, Name: MYERS v. SWINEFORD, Score: 1.5421\n",
      "ID: 8504052, Name: Ex parte DUBUQUE, Score: 1.5421\n",
      "ID: 8504080, Name: GARSIDE v. NORVAL, Score: 1.5421\n",
      "\n",
      "Generating advanced summary...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Query System\n",
    "def query_system():\n",
    "    print(\"Select a query type:\")\n",
    "    print(\"1. Search by Name\")\n",
    "    print(\"2. Search by Abbreviation\")\n",
    "    print(\"3. Search by Decision Date\")\n",
    "    print(\"4. Search by Jurisdiction\")\n",
    "    print(\"5. Custom Legal Query\")\n",
    "\n",
    "    choice = input(\"Enter choice (1-5): \").strip()\n",
    "    query = \"\"\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query = input(\"Enter case name: \").strip()\n",
    "    elif choice == \"2\":\n",
    "        query = input(\"Enter case abbreviation: \").strip()\n",
    "    elif choice == \"3\":\n",
    "        query = input(\"Enter decision date (YYYY-MM-DD): \").strip()\n",
    "    elif choice == \"4\":\n",
    "        query = input(\"Enter jurisdiction: \").strip()\n",
    "    elif choice == \"5\":\n",
    "        query = input(\"Enter your query: \").strip()\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "\n",
    "    results, filtered_results = colbert_retrieve(query, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nRetrieved Cleaned Texts (or Detailed Texts):\")\n",
    "    for res in results:\n",
    "        print(f\"Document ID: {res['id']}, Name: {res['name']}, Cleaned Text Snippet: {res['cleaned_text'][:100]}...\")\n",
    "\n",
    "    print(\"\\nTop results:\")\n",
    "    for res in results:\n",
    "        print(f\"ID: {res['id']}, Name: {res['name']}, Score: {res['score']:.4f}\")\n",
    "\n",
    "    # Advanced summary if any results\n",
    "    if results:\n",
    "        print(\"\\nGenerating advanced summary...\\n\")\n",
    "        advanced_summary = generate_summary(query, results)\n",
    "        print(\"Generated Summary:\")\n",
    "        print(advanced_summary)\n",
    "    else:\n",
    "        print(\"No relevant documents found. Refine your query.\")\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    query_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54faf47-5390-4cca-bca0-90e3a60cccde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c25db-bcfd-47e1-95ee-108990e7ae52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
