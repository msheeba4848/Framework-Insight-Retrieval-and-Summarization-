{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26c951b-c06c-48c4-b1c9-e9f385558f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1.post100)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.13.0,>=0.12.3 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.3.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
      "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
      "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama-index) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.27.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.6.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (10.4.0)\n",
      "Collecting pydantic<2.10.0,>=2.7.0 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (8.5.0)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.3->llama-index) (1.16.0)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.15.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.3->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
      "  Downloading jiter-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.3->llama-index)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.3->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.3->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.3->llama-index) (3.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.3->llama-index) (0.2.0)\n",
      "Downloading llama_index-0.12.3-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_core-0.12.3-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.3.2-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
      "Downloading llama_parse-0.5.16-py3-none-any.whl (14 kB)\n",
      "Downloading openai-1.57.0-py3-none-any.whl (389 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading jiter-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, pydantic-core, jiter, annotated-types, tiktoken, pydantic, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.17\n",
      "    Uninstalling pydantic-1.10.17:\n",
      "      Successfully uninstalled pydantic-1.10.17\n",
      "Successfully installed annotated-types-0.7.0 dirtyjson-1.0.8 filetype-1.2.0 jiter-0.8.0 llama-cloud-0.1.6 llama-index-0.12.3 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-core-0.12.3 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.3.2 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.16 openai-1.57.0 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 striprtf-0.0.26 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers llama-index scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8620fe3-39d0-4638-985e-feb4be1cd74e",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_json_files(folder_path):\n",
    "    \"\"\"Load all JSON files from a given folder into a list of dictionaries.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"json/\"\n",
    "data = load_json_files(folder_path)\n",
    "print(f\"Loaded {len(data)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc14da-22d5-42d1-8a12-c144502413d9",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def preprocess_case_text(text):\n",
    "    \"\"\"Clean and standardize case text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters (keep alphanumeric and legal punctuations)\n",
    "    text = re.sub(r'[^\\w\\s.,;:]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess data by cleaning text and standardizing metadata.\"\"\"\n",
    "    preprocessed_data = []\n",
    "    for case in data:\n",
    "        processed_case = {\n",
    "            \"id\": case.get(\"id\"),\n",
    "            \"name\": case.get(\"name\", \"\").strip(),\n",
    "            \"abbreviation\": case.get(\"abbreviation\", \"\").strip(),\n",
    "            \"decision_date\": case.get(\"decision_date\", \"\").strip(),\n",
    "            \"jurisdiction\": case.get(\"jurisdiction_name\", \"\").strip(),\n",
    "            \"cleaned_text\": preprocess_case_text(case.get(\"case_text\", \"\")),\n",
    "        }\n",
    "        preprocessed_data.append(processed_case)\n",
    "    return preprocessed_data\n",
    "\n",
    "# Example usage\n",
    "preprocessed_data = preprocess_data(data)\n",
    "print(f\"Preprocessed {len(preprocessed_data)} cases.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9758ce7a-d98f-41ec-9eef-eecacb30026b",
   "metadata": {},
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ColBERT:\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        \"\"\"Generate dense embeddings for a given text.\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "            mask = tokens['attention_mask'].squeeze(0).bool()\n",
    "            return token_embeddings[mask].numpy()\n",
    "\n",
    "def generate_embeddings_for_cases(preprocessed_data, output_embedding_file, output_metadata_file):\n",
    "    \"\"\"Generate embeddings for all cases and save to file.\"\"\"\n",
    "    colbert = ColBERT()\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for case in preprocessed_data:\n",
    "        embeddings.append(colbert.generate_embeddings(case[\"cleaned_text\"]))\n",
    "        metadata.append({\n",
    "            \"id\": case[\"id\"],\n",
    "            \"name\": case[\"name\"],\n",
    "            \"abbreviation\": case[\"abbreviation\"],\n",
    "            \"decision_date\": case[\"decision_date\"],\n",
    "            \"jurisdiction\": case[\"jurisdiction\"],\n",
    "            \"cleaned_text\": case[\"cleaned_text\"],  # Include cleaned_text here\n",
    "        })\n",
    "\n",
    "    np.save(output_embedding_file, embeddings, allow_pickle=True)\n",
    "    with open(output_metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    print(f\"Embeddings saved to {output_embedding_file}\")\n",
    "    print(f\"Metadata saved to {output_metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79fd10-a786-4905-8e52-c872e1324d00",
   "metadata": {},
   "source": [
    "# Generate embeddings\n",
    "output_embedding_file = \"data/embeddings.npy\"\n",
    "output_metadata_file = \"data/metadata.json\"\n",
    "generate_embeddings_for_cases(preprocessed_data, output_embedding_file, output_metadata_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4944f2-d569-4952-ab20-ef838f18536b",
   "metadata": {},
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def colbert_retrieve(query, embeddings_file, metadata_file, top_k=5):\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    colbert = ColBERT()\n",
    "    \n",
    "    # Correct tokenization process\n",
    "    query_tokens = colbert.tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        query_outputs = colbert.model(**query_tokens)\n",
    "        query_embeddings = query_outputs.last_hidden_state.squeeze(0)\n",
    "        mask = query_tokens['attention_mask'].squeeze(0).bool()\n",
    "        query_embeddings = query_embeddings[mask].numpy()\n",
    "\n",
    "    scores = []\n",
    "    for doc_embeddings in embeddings:\n",
    "        similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        scores.append(max_similarities.sum())\n",
    "\n",
    "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    results = [\n",
    "        {\n",
    "            \"id\": metadata[i][\"id\"],\n",
    "            \"name\": metadata[i][\"name\"],\n",
    "            \"abbreviation\": metadata[i][\"abbreviation\"],\n",
    "            \"decision_date\": metadata[i][\"decision_date\"],\n",
    "            \"jurisdiction\": metadata[i][\"jurisdiction\"],\n",
    "            \"score\": scores[i],\n",
    "        }\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = \"THE ALASKA GOLD MIN. CO. v. BARBRIDGE et al.\"\n",
    "results = colbert_retrieve(query, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "print(\"Top results:\")\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7ba2a-38ce-4685-8bcd-c239dfb4e4d8",
   "metadata": {},
   "source": [
    "def query_system():\n",
    "    print(\"Select a query type:\")\n",
    "    print(\"1. Search by Name\")\n",
    "    print(\"2. Search by Abbreviation\")\n",
    "    print(\"3. Search by Decision Date\")\n",
    "    print(\"4. Search by Jurisdiction\")\n",
    "    print(\"5. Custom Legal Query\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1-5): \")\n",
    "    query = \"\"\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query = input(\"Enter case name: \")\n",
    "    elif choice == \"2\":\n",
    "        query = input(\"Enter case abbreviation: \")\n",
    "    elif choice == \"3\":\n",
    "        query = input(\"Enter decision date (YYYY-MM-DD): \")\n",
    "    elif choice == \"4\":\n",
    "        query = input(\"Enter jurisdiction: \")\n",
    "    elif choice == \"5\":\n",
    "        query = input(\"Enter custom query: \")\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "\n",
    "    results = colbert_retrieve(query, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "    print(\"Top results:\")\n",
    "    for res in results:\n",
    "        print(f\"ID: {res['id']}, Name: {res['name']}, Score: {res['score']:.4f}\")\n",
    "\n",
    "# Launch the query system\n",
    "query_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d03e7c-b83e-477a-b8fb-baff9c399323",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def generate_summary(query, retrieved_docs):\n",
    "    \"\"\"Generate an advanced summary using RAG.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # Use 'cleaned_text' if available; otherwise, fall back to 'name'\n",
    "    context = \" \".join([doc.get('cleaned_text', doc.get('name', '')) for doc in retrieved_docs])\n",
    "\n",
    "    # Prepare input for the model\n",
    "    input_text = f\"Query: {query} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c617b-2285-4777-919c-9630a97b3414",
   "metadata": {},
   "source": [
    "# Use the retrieved results for summarization\n",
    "retrieved_docs = [res for res in results]  # Retrieve all document metadata\n",
    "if retrieved_docs:\n",
    "    print(\"\\nGenerating advanced summary...\\n\")\n",
    "    advanced_summary = generate_summary(query, retrieved_docs)\n",
    "    print(\"Generated Summary:\")\n",
    "    print(advanced_summary)\n",
    "else:\n",
    "    print(\"No relevant documents found. Refine your query.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81000bab-572d-423c-b023-968d365afa73",
   "metadata": {},
   "source": [
    "print(\"Retrieved Document Example:\")\n",
    "print(retrieved_docs[0])  # Print a sample document to check keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd20e245-40d1-45f5-b739-4a579a781626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: output_cases.csv\n"
     ]
    }
   ],
   "source": [
    "# json to csv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "def json_to_csv(json_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Converts all JSON files in a directory into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        json_dir (str): Path to the directory containing JSON files.\n",
    "        output_csv (str): Path to save the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate over all JSON files in the directory\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_dir, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Flatten the JSON structure and extract relevant data\n",
    "                row = {\n",
    "                    \"id\": data.get(\"id\"),\n",
    "                    \"name\": data.get(\"name\"),\n",
    "                    \"abbreviation\": data.get(\"name_abbreviation\"),\n",
    "                    \"decision_date\": data.get(\"decision_date\"),\n",
    "                    \"court_name\": data.get(\"court\", {}).get(\"name\"),\n",
    "                    \"jurisdiction_name\": data.get(\"jurisdiction\", {}).get(\"name\"),\n",
    "                    \"word_count\": data.get(\"analysis\", {}).get(\"word_count\"),\n",
    "                    \"char_count\": data.get(\"analysis\", {}).get(\"char_count\"),\n",
    "                    \"ocr_confidence\": data.get(\"analysis\", {}).get(\"ocr_confidence\"),\n",
    "                    \"case_text\": \" \".join([opinion[\"text\"] for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])]),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file saved at: {output_csv}\")\n",
    "\n",
    "# Specify the path to the JSON directory and output CSV file\n",
    "json_dir = \"json/\"\n",
    "output_csv = \"output_cases.csv\"\n",
    "\n",
    "# Convert JSON files to CSV\n",
    "json_to_csv(json_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7547b684-a09c-4eca-9a3c-ed75b3712642",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "def load_json_files(folder_path):\n",
    "    \"\"\"Load all JSON files from a given folder into a list of dictionaries.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_case_text(text):\n",
    "    \"\"\"Clean and standardize case text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;:]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess data by cleaning text and standardizing metadata.\"\"\"\n",
    "    preprocessed_data = []\n",
    "    for case in data:\n",
    "        processed_case = {\n",
    "            \"id\": case.get(\"id\"),\n",
    "            \"name\": case.get(\"name\", \"\").strip(),\n",
    "            \"abbreviation\": case.get(\"abbreviation\", \"\").strip(),\n",
    "            \"decision_date\": case.get(\"decision_date\", \"\").strip(),\n",
    "            \"jurisdiction\": case.get(\"jurisdiction_name\", \"\").strip(),\n",
    "            \"cleaned_text\": preprocess_case_text(case.get(\"case_text\", \"\")),\n",
    "        }\n",
    "        preprocessed_data.append(processed_case)\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "# ColBERT Class\n",
    "class ColBERT:\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        \"\"\"Generate dense embeddings for a given text.\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "            mask = tokens['attention_mask'].squeeze(0).bool()\n",
    "            return token_embeddings[mask].numpy()\n",
    "\n",
    "\n",
    "# Generate Embeddings for Cases\n",
    "def generate_embeddings_for_cases(preprocessed_data, output_embedding_file, output_metadata_file):\n",
    "    \"\"\"Generate embeddings for all cases and save to file.\"\"\"\n",
    "    colbert = ColBERT()\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for case in preprocessed_data:\n",
    "        embeddings.append(colbert.generate_embeddings(case[\"cleaned_text\"]))\n",
    "        metadata.append({\n",
    "            \"id\": case[\"id\"],\n",
    "            \"name\": case[\"name\"],\n",
    "            \"abbreviation\": case[\"abbreviation\"],\n",
    "            \"decision_date\": case[\"decision_date\"],\n",
    "            \"jurisdiction\": case[\"jurisdiction\"],\n",
    "            \"cleaned_text\": case[\"cleaned_text\"],  # Include cleaned_text here\n",
    "        })\n",
    "\n",
    "    np.save(output_embedding_file, embeddings, allow_pickle=True)\n",
    "    with open(output_metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    print(f\"Embeddings saved to {output_embedding_file}\")\n",
    "    print(f\"Metadata saved to {output_metadata_file}\")\n",
    "\n",
    "\n",
    "# ColBERT Retrieval\n",
    "def colbert_retrieve(query, embeddings_file, metadata_file, top_k=5):\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    colbert = ColBERT()\n",
    "    query_tokens = colbert.tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        query_outputs = colbert.model(**query_tokens)\n",
    "        query_embeddings = query_outputs.last_hidden_state.squeeze(0)\n",
    "        mask = query_tokens['attention_mask'].squeeze(0).bool()\n",
    "        query_embeddings = query_embeddings[mask].numpy()\n",
    "\n",
    "    scores = []\n",
    "    for doc_embeddings in embeddings:\n",
    "        similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        scores.append(max_similarities.sum())\n",
    "\n",
    "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    results = [\n",
    "        {\n",
    "            \"id\": metadata[i][\"id\"],\n",
    "            \"name\": metadata[i][\"name\"],\n",
    "            \"abbreviation\": metadata[i][\"abbreviation\"],\n",
    "            \"decision_date\": metadata[i][\"decision_date\"],\n",
    "            \"jurisdiction\": metadata[i][\"jurisdiction\"],\n",
    "            \"cleaned_text\": metadata[i][\"cleaned_text\"],\n",
    "            \"score\": scores[i],\n",
    "        }\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    return results\n",
    "\n",
    "\n",
    "# Summarization with RAG\n",
    "def generate_summary(query, retrieved_docs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    context = \" \".join([doc.get('cleaned_text', '') for doc in retrieved_docs])\n",
    "    input_text = f\"Query: {query} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Query System\n",
    "def query_system():\n",
    "    print(\"Select a query type:\")\n",
    "    print(\"1. Search by Name\")\n",
    "    print(\"2. Search by Abbreviation\")\n",
    "    print(\"3. Search by Decision Date\")\n",
    "    print(\"4. Search by Jurisdiction\")\n",
    "    print(\"5. Custom Legal Query\")\n",
    "\n",
    "    choice = input(\"Enter choice (1-5): \")\n",
    "    query = \"\"\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query = input(\"Enter case name: \")\n",
    "    elif choice == \"2\":\n",
    "        query = input(\"Enter case abbreviation: \")\n",
    "    elif choice == \"3\":\n",
    "        query = input(\"Enter decision date (YYYY-MM-DD): \")\n",
    "    elif choice == \"4\":\n",
    "        query = input(\"Enter jurisdiction: \")\n",
    "    elif choice == \"5\":\n",
    "        query = input(\"Enter custom query: \")\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "\n",
    "    results = colbert_retrieve(query, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "    print(\"Top results:\")\n",
    "    for res in results:\n",
    "        print(f\"ID: {res['id']}, Name: {res['name']}, Score: {res['score']:.4f}\")\n",
    "\n",
    "    if results:\n",
    "        print(\"\\nGenerating advanced summary...\\n\")\n",
    "        advanced_summary = generate_summary(query, results)\n",
    "        print(\"Generated Summary:\")\n",
    "        print(advanced_summary)\n",
    "    else:\n",
    "        print(\"No relevant documents found. Refine your query.\")\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"json/\"\n",
    "    data = load_json_files(folder_path)\n",
    "    preprocessed_data = preprocess_data(data)\n",
    "    generate_embeddings_for_cases(preprocessed_data, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "    query_system()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7970cfb-4955-4feb-9970-6a5eb11b7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "def load_json_files(folder_path):\n",
    "    \"\"\"Load all JSON files from a given folder into a list of dictionaries.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Extract and Clean Case Text\n",
    "def extract_case_text(case):\n",
    "    \"\"\"Extract and clean the case text.\"\"\"\n",
    "    opinions = case.get(\"casebody\", {}).get(\"opinions\", [])\n",
    "    if opinions:\n",
    "        return re.sub(r'\\s+', ' ', opinions[0].get(\"text\", \"\").strip())  # Get first opinion's text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocess data by cleaning text and standardizing metadata.\"\"\"\n",
    "    preprocessed_data = []\n",
    "    for case in data:\n",
    "        processed_case = {\n",
    "            \"id\": case.get(\"id\"),\n",
    "            \"name\": case.get(\"name\", \"\").strip(),\n",
    "            \"abbreviation\": case.get(\"name_abbreviation\", \"\").strip(),\n",
    "            \"decision_date\": case.get(\"decision_date\", \"\").strip(),\n",
    "            \"jurisdiction\": case.get(\"jurisdiction\", {}).get(\"name\", \"\").strip(),\n",
    "            \"cleaned_text\": extract_case_text(case),\n",
    "        }\n",
    "        preprocessed_data.append(processed_case)\n",
    "    return preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fdd6abc-8916-48e1-a5da-6528769c754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT Class\n",
    "class ColBERT:\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        \"\"\"Generate dense embeddings for a given text.\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "            mask = tokens['attention_mask'].squeeze(0).bool()\n",
    "            return token_embeddings[mask].numpy()\n",
    "\n",
    "\n",
    "# ColBERT Retrieval\n",
    "def colbert_retrieve(query, embeddings_file, metadata_file, top_k=5):\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    colbert = ColBERT()\n",
    "    query_tokens = colbert.tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        query_outputs = colbert.model(**query_tokens)\n",
    "        query_embeddings = query_outputs.last_hidden_state.squeeze(0)\n",
    "        mask = query_tokens['attention_mask'].squeeze(0).bool()\n",
    "        query_embeddings = query_embeddings[mask].numpy()\n",
    "\n",
    "    scores = []\n",
    "    for doc_embeddings in embeddings:\n",
    "        similarity_matrix = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "        max_similarities = similarity_matrix.max(axis=1)\n",
    "        scores.append(max_similarities.sum())\n",
    "\n",
    "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    results = [\n",
    "        {\n",
    "            \"id\": metadata[i][\"id\"],\n",
    "            \"name\": metadata[i][\"name\"],\n",
    "            \"abbreviation\": metadata[i][\"abbreviation\"],\n",
    "            \"decision_date\": metadata[i][\"decision_date\"],\n",
    "            \"jurisdiction\": metadata[i][\"jurisdiction\"],\n",
    "            \"cleaned_text\": metadata[i][\"cleaned_text\"],\n",
    "            \"score\": scores[i],\n",
    "        }\n",
    "        for i in top_indices\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30e3ea5d-76b7-492f-8f94-a594153b0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization with RAG\n",
    "def generate_summary(query, retrieved_docs):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    context = \" \".join([doc.get('cleaned_text', '') for doc in retrieved_docs])\n",
    "    input_text = f\"Query: {query} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a993d9d-babe-4912-a1eb-a4bd990b0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Search by Jurisdiction\n",
      "5. Custom Legal Query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-5):  5\n",
      "Enter custom query:  What about GARSIDE v. NORVAL?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "ID: 8503986, Name: In re JESSE SCOTT OLIVER, Minor, Score: 3.6140\n",
      "ID: 8506137, Name: UNITED STATES v. SHEEP CREEK JOHN, Score: 3.6140\n",
      "ID: 8504379, Name: PRATT et al. v. UNITED ALASKA MIN. CO., Score: 3.6140\n",
      "ID: 8504361, Name: SUTTER et al. v. HECKMAN et al., Score: 3.6140\n",
      "ID: 8504334, Name: In re THOMPKINS McINTIRE ESTATE, Score: 3.6140\n",
      "\n",
      "Generating advanced summary...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Query: What about GARSIDE v. NORVAL? Context:    v. GARSide v.NORVAL? context:  GARSIDE vs. NOR VAL. context: Garside vs. Norval. Context: NORVAL vs. Garside. context : \n"
     ]
    }
   ],
   "source": [
    "# Query System\n",
    "def query_system():\n",
    "    print(\"Select a query type:\")\n",
    "    print(\"1. Search by Name\")\n",
    "    print(\"2. Search by Abbreviation\")\n",
    "    print(\"3. Search by Decision Date\")\n",
    "    print(\"4. Search by Jurisdiction\")\n",
    "    print(\"5. Custom Legal Query\")\n",
    "\n",
    "    choice = input(\"Enter choice (1-5): \")\n",
    "    query = \"\"\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query = input(\"Enter case name: \")\n",
    "    elif choice == \"2\":\n",
    "        query = input(\"Enter case abbreviation: \")\n",
    "    elif choice == \"3\":\n",
    "        query = input(\"Enter decision date (YYYY-MM-DD): \")\n",
    "    elif choice == \"4\":\n",
    "        query = input(\"Enter jurisdiction: \")\n",
    "    elif choice == \"5\":\n",
    "        query = input(\"Enter custom query: \")\n",
    "    else:\n",
    "        print(\"Invalid choice. Exiting.\")\n",
    "        return\n",
    "\n",
    "    results = colbert_retrieve(query, \"data/embeddings.npy\", \"data/metadata.json\")\n",
    "    print(\"Top results:\")\n",
    "    for res in results:\n",
    "        print(f\"ID: {res['id']}, Name: {res['name']}, Score: {res['score']:.4f}\")\n",
    "\n",
    "    if results:\n",
    "        print(\"\\nGenerating advanced summary...\\n\")\n",
    "        advanced_summary = generate_summary(query, results)\n",
    "        print(\"Generated Summary:\")\n",
    "        print(advanced_summary)\n",
    "    else:\n",
    "        print(\"No relevant documents found. Refine your query.\")\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"json/\"\n",
    "    data = load_json_files(folder_path)\n",
    "    preprocessed_data = preprocess_data(data)\n",
    "    # Assuming embeddings and metadata are already generated\n",
    "    query_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaffac5-8c08-46db-a029-4a0b1123f0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
