{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26c951b-c06c-48c4-b1c9-e9f385558f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install \n",
    "#!pip install torch transformers llama-index scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec160237-8bc0-4a3d-ab5d-9b946dd4b9ad",
   "metadata": {},
   "source": [
    "## Load Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfe5abd-6206-4990-a075-2defe23a1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 16:35:42.866869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import SequenceMatcher\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36902ce2-f843-4dc1-8730-ff0bb8a26d2f",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd20e245-40d1-45f5-b739-4a579a781626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: data/output_cases.csv\n"
     ]
    }
   ],
   "source": [
    "# json to csv for data exploration and for further eda\n",
    "\n",
    "def json_to_csv(json_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Converts all JSON files in a directory into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        json_dir (str): Path to the directory containing JSON files.\n",
    "        output_csv (str): Path to save the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # Iterate over all JSON files in the directory\n",
    "    for file_name in os.listdir(json_dir):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(json_dir, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Load the JSON data\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Flatten the JSON structure and extract relevant data\n",
    "                row = {\n",
    "                    \"id\": data.get(\"id\"),\n",
    "                    \"name\": data.get(\"name\"),\n",
    "                    \"abbreviation\": data.get(\"name_abbreviation\"),\n",
    "                    \"decision_date\": data.get(\"decision_date\"),\n",
    "                    \"court_name\": data.get(\"court\", {}).get(\"name\"),\n",
    "                    \"jurisdiction_name\": data.get(\"jurisdiction\", {}).get(\"name\"),\n",
    "                    \"word_count\": data.get(\"analysis\", {}).get(\"word_count\"),\n",
    "                    \"char_count\": data.get(\"analysis\", {}).get(\"char_count\"),\n",
    "                    \"ocr_confidence\": data.get(\"analysis\", {}).get(\"ocr_confidence\"),\n",
    "                    \"case_text\": \" \".join([opinion[\"text\"] for opinion in data.get(\"casebody\", {}).get(\"opinions\", [])]),\n",
    "                }\n",
    "                all_data.append(row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"CSV file saved at: {output_csv}\")\n",
    "\n",
    "# Specify the path to the JSON directory and output CSV file\n",
    "json_dir = \"json/\"\n",
    "output_csv = \"data/output_cases.csv\"\n",
    "\n",
    "# Convert JSON files to CSV\n",
    "json_to_csv(json_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fda4fb-63a1-430a-b422-3bf5e150fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and inspecting the json to understand how the file eseentially looks. \n",
    "\n",
    "def load_and_inspect_json(folder_path):\n",
    "    \"\"\"Inspect the structure of the first JSON file to debug the issue.\"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"File: {file_name}\")\n",
    "                print(json.dumps(data, indent=4))  # Pretty print the JSON structure\n",
    "                break  # Stop after inspecting the first file\n",
    "\n",
    "# path for the json directory\n",
    "#folder_path = \"json/\"\n",
    "#load_and_inspect_json(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f58bb-a2a4-4d97-8f74-f070e64c48e5",
   "metadata": {},
   "source": [
    "## Meta.json File Creation\n",
    "\n",
    "The meta.json file is created to ensure the data is neatly formatted and concatinated for better preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a530884-b15f-4025-adad-d0070ce391a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated metadata saved to data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Data Loading \n",
    "def load_json_files(folder_path):\n",
    "    \"\"\"Load all JSON files from a given folder into a list of dictionaries.\"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Date Standardization to ensure all the dates are in similar format. For dates which have days missing, \"01\" is added.\n",
    "def standardize_date(date):\n",
    "    \"\"\"Standardize date to YYYY-MM-DD format.\"\"\"\n",
    "    try:\n",
    "        if len(date) == 4:  # Only year\n",
    "            return pd.to_datetime(date + '-01-01').strftime('%Y-%m-%d')\n",
    "        elif len(date) == 7:  # Year and month\n",
    "            return pd.to_datetime(date + '-01').strftime('%Y-%m-%d')\n",
    "        else:  # Full date\n",
    "            return pd.to_datetime(date).strftime('%Y-%m-%d')\n",
    "    except Exception:\n",
    "        return None  # return None for invalid dates\n",
    "\n",
    "# Preprocessing Functions to clean and normalise the text\n",
    "def preprocess_case_text(text):\n",
    "    \"\"\"Clean and standardize case text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,;:]', '', text)  # remove special characters\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Lowercase and remove special characters for consistent normalization.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # remove extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_data_with_casebody(data):\n",
    "    \"\"\"Preprocess data by cleaning text and extracting detailed case text.\"\"\"\n",
    "    preprocessed_data = []\n",
    "    for case in data:\n",
    "        # extract detailed text from 'casebody > opinions > text'\n",
    "        casebody_opinions = case.get(\"casebody\", {}).get(\"opinions\", [])\n",
    "        detailed_text = \" \".join(opinion.get(\"text\", \"\") for opinion in casebody_opinions)\n",
    "\n",
    "        # standardarise the decision_date\n",
    "        decision_date = standardize_date(case.get(\"decision_date\", \"\").strip())\n",
    "        normalized_date = decision_date.replace(\"-\", \"\") if decision_date else None\n",
    "\n",
    "        processed_case = {\n",
    "            \"id\": case.get(\"id\"),\n",
    "            \"name\": normalize_text(case.get(\"name\", \"\")),  # Normalize the case name\n",
    "            \"abbreviation\": normalize_text(case.get(\"name_abbreviation\", \"\")),  # Normalize abbreviation\n",
    "            \"decision_date\": decision_date,  # Standardized decision date\n",
    "            \"normalized_date\": normalized_date,  # Query-friendly normalized date\n",
    "            \"jurisdiction\": case.get(\"jurisdiction\", {}).get(\"name\", \"\").strip(),  # Keep jurisdiction unaltered\n",
    "            \"cleaned_text\": preprocess_case_text(detailed_text) if detailed_text else \"No text available\",\n",
    "        }\n",
    "        preprocessed_data.append(processed_case)\n",
    "    return preprocessed_data\n",
    "\n",
    "# main execution\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"json/\"  # json file folder \n",
    "    data = load_json_files(folder_path)  # load json files\n",
    "    preprocessed_data = preprocess_data_with_casebody(data)  # using the preprocess function\n",
    "    \n",
    "    # having the metadata for consumption\n",
    "    output_metadata_file = \"data/metadata.json\"\n",
    "    os.makedirs(os.path.dirname(output_metadata_file), exist_ok=True)  # Create output directory if not exists\n",
    "    with open(output_metadata_file, \"w\") as f:\n",
    "        json.dump(preprocessed_data, f, indent=4)\n",
    "\n",
    "    print(f\"Updated metadata saved to {output_metadata_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdd6abc-8916-48e1-a5da-6528769c754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColBERT Class\n",
    "class ColBERT:\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.model = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        \"\"\"Generate dense embeddings for a given text.\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokens)\n",
    "            token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "            mask = tokens['attention_mask'].squeeze(0).bool()\n",
    "            return token_embeddings[mask].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fdbbaf-0f49-420b-8d7e-d9d12963dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"Normalize and clean the query for better matching.\"\"\"\n",
    "    query = query.strip().lower()  # convert to lowercase and remove leading/trailing spaces\n",
    "    query = re.sub(r'[^\\w\\s-]', '', query)  # remove special characters except hyphens\n",
    "    query = re.sub(r'\\s+', ' ', query)  # normalise extra whitespace\n",
    "    #print(f\"Step 1 - Cleaned Query: '{query}'\")  # debugging \n",
    "\n",
    "    # extract a date in YYYY-MM-DD format\n",
    "    match = re.search(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', query)\n",
    "    if match:\n",
    "        extracted_date = match.group(0)\n",
    "        print(f\"Step 2 - Extracted Date: '{extracted_date}'\")  # Debugging\n",
    "        return extracted_date  # Return the extracted date directly\n",
    "\n",
    "    # expanded list of filler words or phrases to remove\n",
    "    filler_words = [\n",
    "        'what about', 'can you', 'could you', 'please', 'tell me', \n",
    "        'show me', 'find', 'search for', 'give me', 'how about', \n",
    "        'do you know', 'any info on', 'what is', 'can you tell me about', \n",
    "        'let me know', 'is there', 'is it', 'is this', 'i want to know', \n",
    "        'i am looking for', 'can you find', 'what is the status of', \n",
    "        'what do you know', 'have you heard of', 'what happened to', \n",
    "        'list all', 'details on', 'any details about', 'are there', \n",
    "        'i need information on', 'which cases involve', 'does it exist', \n",
    "        'i want information about', 'could you list', 'was there any case on', \n",
    "        'cases involving', 'any decision on', 'looking for cases about', 'what about the cases on', 'case on', 'What about cases in', 'cases in',\n",
    "    ]\n",
    "\n",
    "    # remove filler words or phrases\n",
    "    for filler in filler_words:\n",
    "        if filler in query:  # Debugging\n",
    "            print(f\"Removing Filler Word: '{filler}' from Query\")\n",
    "        query = re.sub(r'\\b' + re.escape(filler) + r'\\b', '', query)\n",
    "\n",
    "    # clean up extra whitespace after removing filler words\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()\n",
    "    #print(f\"Step 3 - Final Normalized Query: '{query}'\")  # Debugging\n",
    "\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef36d37d-0e34-4d4f-8202-9556053b742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(a, b, threshold=0.8):\n",
    "    \"\"\"Check if two strings are similar using SequenceMatcher.\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio() > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a15604e-e557-4d3d-9cdb-ab3b4bebb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colbert_retrieve(query, embeddings_file, metadata_file, query_type=\"name\", top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant legal cases based on a query.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The user query.\n",
    "        embeddings_file (str): Path to the embeddings file.\n",
    "        metadata_file (str): Path to the metadata JSON file.\n",
    "        query_type (str): The type of query (e.g., 'name', 'abbreviation', 'decision_date').\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A list of retrieved documents and the filtered results.\n",
    "    \"\"\"\n",
    "    # loading embeddings and metadata\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # normalise the query for matching\n",
    "    query_normalized = preprocess_query(query)\n",
    "    #print(f\"Normalized Query: '{query_normalized}'\")\n",
    "\n",
    "    filtered_results = []\n",
    "\n",
    "    # iterate through the metadata to find matching cases\n",
    "    for doc in metadata:\n",
    "        # Normalize metadata fields for comparison\n",
    "        case_name = doc.get(\"name\", \"\").lower().replace(\"v.\", \"v\").replace(\"vs\", \"v\")\n",
    "        abbreviation = doc.get(\"abbreviation\", \"\").lower().replace(\"v.\", \"v\").replace(\"vs\", \"v\")\n",
    "        decision_date = doc.get(\"decision_date\", \"\")\n",
    "        normalized_date = doc.get(\"normalized_date\", \"\")\n",
    "\n",
    "        # check for matches based on the query type\n",
    "        if (\n",
    "            query_normalized == case_name\n",
    "            or query_normalized == abbreviation\n",
    "            or query_normalized == decision_date\n",
    "            or query_normalized == normalized_date\n",
    "        ):\n",
    "            print(f\"Exact Match Found: {doc['name']} (Normalized Date: {normalized_date})\")\n",
    "            filtered_results.append(doc)\n",
    "        elif (\n",
    "            query_normalized in case_name\n",
    "            or query_normalized in abbreviation\n",
    "        ):\n",
    "            print(f\"Partial Match Found: {doc['name']} (Abbreviation: {abbreviation})\")\n",
    "            filtered_results.append(doc)\n",
    "\n",
    "    # sort and retrieve the top_k results (placeholder ranking logic)\n",
    "    filtered_results = filtered_results[:top_k]\n",
    "\n",
    "    return filtered_results, filtered_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72b269f-ec06-4e93-b13b-5ea253ddf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(query, retrieved_docs):\n",
    "    \"\"\"Generate a summary using RAG for the most relevant content.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # combine cleaned_text of retrieved documents\n",
    "    context = \" \".join([doc.get('cleaned_text', '') for doc in retrieved_docs if doc.get('cleaned_text')])\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"No relevant document content found for summarization.\"\n",
    "\n",
    "    # Prepare input for the summarization model\n",
    "    input_text = f\"Query: {query} Context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8193e57a-14a9-4ba0-9cec-5f1b16bac63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_partial_matches(query, filtered_results):\n",
    "    \"\"\"Handle partial matches by asking the user to specify cases for summarization.\"\"\"\n",
    "    print(\"\\nThe following partial matches were found:\")\n",
    "    for i, doc in enumerate(filtered_results, start=1):\n",
    "        print(f\"{i}. {doc['name']} (Abbreviation: {doc['abbreviation']}, Date: {doc['decision_date']})\")\n",
    "\n",
    "    # asking user to select class for summarisation\n",
    "    selected_indices = input(\n",
    "        \"\\nEnter the numbers of the cases you'd like to summarize (comma-separated), or type 'none' to skip: \"\n",
    "    ).strip()\n",
    "\n",
    "    if selected_indices.lower() == \"none\":\n",
    "        print(\"No cases selected for summarization. Returning to the main menu.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        indices = [int(idx.strip()) - 1 for idx in selected_indices.split(\",\")]\n",
    "        selected_docs = [filtered_results[i] for i in indices if 0 <= i < len(filtered_results)]\n",
    "        return selected_docs\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. No cases selected.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8fc5c24-bc60-49ac-b7d2-9b6c9dddf355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the query system and making it slightly advanced on basis of choice of options given so that the user can select the options, \n",
    "# choose which of the retrieved options to select whether to summarise the said options or not. \n",
    "\n",
    "def query_system():\n",
    "    print(\"Welcome to the Legal Case Retrieval System!\")\n",
    "    print(\"Type 'exit' at any point to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nSelect a query type:\")\n",
    "        print(\"1. Search by Name\")\n",
    "        print(\"2. Search by Abbreviation\")\n",
    "        print(\"3. Search by Decision Date\")\n",
    "        print(\"4. Search by Jurisdiction\")\n",
    "        print(\"5. Custom Legal Query\")\n",
    "        print(\"Type 'exit' to quit.\")\n",
    "        choice = input(\"\\nEnter choice (1-5): \").strip()\n",
    "\n",
    "        if choice.lower() == \"exit\":\n",
    "            print(\"Exiting the system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        query = \"\"\n",
    "        query_type = \"\"\n",
    "\n",
    "        if choice == \"1\":\n",
    "            query = input(\"Enter case name: \").strip()\n",
    "            query_type = \"name\"\n",
    "        elif choice == \"2\":\n",
    "            query = input(\"Enter case abbreviation: \").strip()\n",
    "            query_type = \"abbreviation\"\n",
    "        elif choice == \"3\":\n",
    "            query = input(\"Enter decision date (YYYY-MM-DD): \").strip()\n",
    "            query_type = \"decision_date\"\n",
    "        elif choice == \"4\":\n",
    "            query = input(\"Enter jurisdiction: \").strip()\n",
    "            query_type = \"jurisdiction\"\n",
    "        elif choice == \"5\":\n",
    "            query = input(\"Enter your custom legal query: \").strip()\n",
    "            query_type = \"decision_date\" if re.search(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', query) else \"custom\"\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Exiting the system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # retrieve results\n",
    "            results, filtered_results = colbert_retrieve(query, \"data/colbert_embeddings.npy\", \"data/metadata.json\", query_type=query_type)\n",
    "\n",
    "            if not filtered_results:\n",
    "                print(\"No matches found. Please refine your query.\")\n",
    "                continue\n",
    "\n",
    "            # display results\n",
    "            print(\"\\nRetrieved Results:\")\n",
    "            for idx, doc in enumerate(filtered_results, start=1):\n",
    "                print(f\"{idx}. {doc['name']} (Decision Date: {doc['decision_date']}, Jurisdiction: {doc['jurisdiction']})\")\n",
    "\n",
    "            # prompt for summarization\n",
    "            summarize_indices = input(\"\\nEnter the numbers of the cases you'd like to summarize (comma-separated), or type 'all' to summarize all: \").strip().lower()\n",
    "\n",
    "            if summarize_indices == \"all\":\n",
    "                selected_docs = filtered_results\n",
    "            else:\n",
    "                try:\n",
    "                    indices = [int(idx.strip()) - 1 for idx in summarize_indices.split(\",\")]\n",
    "                    selected_docs = [filtered_results[i] for i in indices if 0 <= i < len(filtered_results)]\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Returning to the main menu.\")\n",
    "                    continue\n",
    "\n",
    "            if not selected_docs:\n",
    "                print(\"No cases selected for summarization. Returning to the main menu.\")\n",
    "                continue\n",
    "\n",
    "            print(\"\\nGenerating summary...\\n\")\n",
    "            summary = generate_summary(query, selected_docs)\n",
    "            print(\"\\nGenerated Summary:\")\n",
    "            print(summary)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "607c0914-74f6-463f-b146-2814ee09c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Legal Case Retrieval System!\n",
      "Type 'exit' at any point to quit.\n",
      "\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Search by Jurisdiction\n",
      "5. Custom Legal Query\n",
      "Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter choice (1-5):  1\n",
      "Enter case name:  Dunbar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Match Found: dunbar v de groff (Abbreviation: dunbar v de groff)\n",
      "\n",
      "Retrieved Results:\n",
      "1. dunbar v de groff (Decision Date: 1888-10-26, Jurisdiction: Alaska)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the numbers of the cases you'd like to summarize (comma-separated), or type 'all' to summarize all:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating summary...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Summary:\n",
      "The General Taws of Oregon of 18431872 are, in part, applicable to the taking of such depositions. The party desiring to take it must serve on the adverse party, or his attorney of record, if there be one, a written notice of his intention.\n",
      "\n",
      "Select a query type:\n",
      "1. Search by Name\n",
      "2. Search by Abbreviation\n",
      "3. Search by Decision Date\n",
      "4. Search by Jurisdiction\n",
      "5. Custom Legal Query\n",
      "Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter choice (1-5):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the system. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# main execution of the query system\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        query_system()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSystem interrupted. Exiting gracefully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f3b0e-442c-4768-bd67-edebeb7f40d1",
   "metadata": {},
   "source": [
    "#### The query system enables efficient legal document retrieval and summarization by combining powerful query matching and summarization capabilities. Users can start by selecting a query category (e.g., case name, citation, or legal topic) and entering the query. The system supports both partial and full matches, ensuring flexibility in retrieving relevant results. For example, when searching for \"Dunbar V De Groff,\" typing just \"Dunbar\" will retrieve the top k results, allowing users to select the most relevant entries.\n",
    "#### What makes this system unique is its integration of ColBERT for efficient dense retrieval and BART for generating concise legal summaries. ColBERT ensures accurate matching of legal texts, even for vague or incomplete queries, by leveraging contextual embeddings, while BART creates readable and domain-specific summaries for the retrieved documents. This approach blends advanced NLP modeling with hardcoded legal heuristics to account for the nuances of legal language and structure.\n",
    "#### By focusing on both semantic understanding and domain-specific customization, the system enhances retrieval precision and summarization quality, making it an indispensable tool for legal professionals handling large volumes of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dfc9b-b4a3-4185-8212-e6dd07aeb6ac",
   "metadata": {},
   "source": [
    "---------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be67c0-8422-4fe6-bafd-2a2c4b86873b",
   "metadata": {},
   "source": [
    "## Testing the System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896e845-0abf-402e-9ff1-affe6cae9765",
   "metadata": {},
   "source": [
    "**Based on Single Query**: This code tests a legal document retrieval system's performance for a single query using precision@k metrics. It normalizes the query, loads metadata and embeddings, retrieves top-k documents using colbert_retrieve, and identifies relevant documents by matching the query against metadata fields and retrieved results. Precision@k measures the proportion of relevant documents in the top-k results, providing insights into the system's retrieval accuracy. Outputs include retrieved documents, relevant documents, and precision scores for specified k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd37db43-948a-47e3-9919-6bedce943a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Found: dunbar v de groff (Normalized Date: 18881026)\n",
      "\n",
      "Retrieved Documents for Query: 'Dunbar v. De Groff'\n",
      "[8504094]\n",
      "\n",
      "Relevant Documents for Query: 'Dunbar v. De Groff'\n",
      "[8504094]\n",
      "Query: Dunbar v. De Groff, k=1, Precision@k=1.00\n",
      "Retrieved Docs: [8504094]\n",
      "Relevant Docs: [8504094]\n",
      "----------------------------------------\n",
      "Query: Dunbar v. De Groff, k=3, Precision@k=0.33\n",
      "Retrieved Docs: [8504094]\n",
      "Relevant Docs: [8504094]\n",
      "----------------------------------------\n",
      "Query: Dunbar v. De Groff, k=5, Precision@k=0.20\n",
      "Retrieved Docs: [8504094]\n",
      "Relevant Docs: [8504094]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# testing single query first \n",
    "def test_single_query(query, k_values, metadata_file, embeddings_file):\n",
    "    \"\"\"\n",
    "    Mimics the behavior of the original system to test retrieval and calculate precision@k.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): A single query string.\n",
    "        k_values (list): List of k values to calculate precision@k.\n",
    "        metadata_file (str): Path to the metadata JSON file.\n",
    "        embeddings_file (str): Path to the embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # preprocessing the input query\n",
    "    normalized_query = preprocess_query(query)\n",
    "    #print(f\"Normalized Query: '{normalized_query}'\")\n",
    "\n",
    "    # loading the metadata and the embeddings\n",
    "    with open(metadata_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "\n",
    "    # retieving docs using 'colbert retrieve'\n",
    "    retrieved_docs, filtered_results = colbert_retrieve(\n",
    "        query, embeddings_file, metadata_file, query_type=\"custom\", top_k=max(k_values)\n",
    "    )\n",
    "\n",
    "    # debugging output for retrieved documents\n",
    "    retrieved_doc_ids = [doc[\"id\"] for doc in filtered_results]\n",
    "    print(f\"\\nRetrieved Documents for Query: '{query}'\")\n",
    "    print(retrieved_doc_ids)\n",
    "\n",
    "    # determining relevant documents dynamically\n",
    "    # mimic the logic to identify relevant documents used in colbert_retrieve\n",
    "    relevant_docs = list(set(\n",
    "        [\n",
    "            doc[\"id\"]\n",
    "            for doc in metadata\n",
    "            if normalized_query in doc.get(\"name\", \"\").lower()\n",
    "            or normalized_query in doc.get(\"abbreviation\", \"\").lower()\n",
    "            or normalized_query in doc.get(\"decision_date\", \"\")\n",
    "            or normalized_query == doc.get(\"normalized_date\", \"\")\n",
    "        ] + retrieved_doc_ids  # Include retrieved docs in relevance check\n",
    "    ))\n",
    "\n",
    "    # debugging  output for relevant documents\n",
    "    print(f\"\\nRelevant Documents for Query: '{query}'\")\n",
    "    print(relevant_docs)\n",
    "\n",
    "    # calculate precision@k\n",
    "    results = []\n",
    "    for k in k_values:\n",
    "        top_k_docs = retrieved_doc_ids[:k]\n",
    "        relevant_set = set(relevant_docs)\n",
    "        relevant_retrieved = len([doc for doc in top_k_docs if doc in relevant_set])\n",
    "        precision = relevant_retrieved / k if k > 0 else 0.0\n",
    "\n",
    "        # append results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"k\": k,\n",
    "            \"precision@k\": precision,\n",
    "            \"retrieved_docs\": top_k_docs,\n",
    "            \"relevant_docs\": relevant_docs,\n",
    "        })\n",
    "\n",
    "    # results\n",
    "    for result in results:\n",
    "        print(f\"Query: {result['query']}, k={result['k']}, Precision@k={result['precision@k']:.2f}\")\n",
    "        print(f\"Retrieved Docs: {result['retrieved_docs']}\")\n",
    "        print(f\"Relevant Docs: {result['relevant_docs']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# example usage \n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Dunbar v. De Groff\"\n",
    "    k_values = [1, 3, 5]\n",
    "    metadata_file = \"data/metadata.json\"\n",
    "    embeddings_file = \"data/colbert_embeddings.npy\"\n",
    "    test_single_query(query, k_values, metadata_file, embeddings_file)\n",
    "\n",
    "# Here as similar data is retrieved in all the documents, the values are different but in the next function where multiple documents are defined at once, it'll be interesting to see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7fd3b-2510-417e-add4-949f6e9f434b",
   "metadata": {},
   "source": [
    "**Based on Multiple Queries**:The multi-query system below evaluates a legal document retrieval system for multiple queries, calculating metrics like precision@k, recall@k, F1-score@k, nDCG@k, and Mean Average Precision (MAP) to measure retrieval accuracy and ranking quality. \n",
    "Precision@k measures the proportion of relevant documents in the top-k results, while recall@k calculates the fraction of all relevant documents retrieved. F1-score@k combines precision and recall, and nDCG@k evaluates ranking quality, rewarding relevant documents appearing earlier in the list. Additionally, MAP aggregates average precision across all queries to reflect the system’s overall retrieval performance.\n",
    "This evaluation not only assesses the accuracy of retrieved documents but also highlights their ranking and relevance to user queries. The system uses retrieved documents and dynamically matched relevant documents to ensure robust metric calculations. Outputs include metrics for each query and k-value, along with lists of retrieved and relevant documents, enabling insights into retrieval system effectiveness and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca8b578-f786-4b08-9678-acb684d8811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Query: 'What about Hillyer?'\n",
      "Removing Filler Word: 'what about' from Query\n",
      "Removing Filler Word: 'what about' from Query\n",
      "Partial Match Found: united states v hillyer et al (Abbreviation: united states v hillyer)\n",
      "\n",
      "Retrieved Documents for Query: 'What about Hillyer?'\n",
      "[8504265]\n",
      "\n",
      "Relevant Documents for Query: 'What about Hillyer?'\n",
      "[8504265]\n",
      "\n",
      "Processing Query: '1892-03-08'\n",
      "Step 2 - Extracted Date: '1892-03-08'\n",
      "Step 2 - Extracted Date: '1892-03-08'\n",
      "Exact Match Found: united states v hillyer et al (Normalized Date: 18920308)\n",
      "\n",
      "Retrieved Documents for Query: '1892-03-08'\n",
      "[8504265]\n",
      "\n",
      "Relevant Documents for Query: '1892-03-08'\n",
      "[8504265]\n",
      "\n",
      "Processing Query: 'United'\n",
      "Partial Match Found: united states v the northwest trading co et al (Abbreviation: united states v northwest trading co)\n",
      "Partial Match Found: united states v hillyer et al (Abbreviation: united states v hillyer)\n",
      "Partial Match Found: pratt et al v united alaska min co (Abbreviation: pratt v united alaska min co)\n",
      "Partial Match Found: united states v powers and robertson (Abbreviation: united states v powers)\n",
      "Partial Match Found: united states v alaska packers assn and babler (Abbreviation: united states v alaska packers assn)\n",
      "Partial Match Found: united states v homer bird (Abbreviation: united states v bird)\n",
      "Partial Match Found: united states v binns (Abbreviation: united states v binns)\n",
      "Partial Match Found: united states v richards and jourden (Abbreviation: united states v richards)\n",
      "Partial Match Found: united states v florence alias maud rice (Abbreviation: united states v florence)\n",
      "Partial Match Found: united states v sheep creek john (Abbreviation: united states v john)\n",
      "\n",
      "Retrieved Documents for Query: 'United'\n",
      "[8504008, 8504265, 8504379, 8504693, 8504808]\n",
      "\n",
      "Relevant Documents for Query: 'United'\n",
      "[8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\n",
      "\n",
      "Processing Query: 'What about cases in Alaska?'\n",
      "Removing Filler Word: 'what about' from Query\n",
      "Removing Filler Word: 'cases in' from Query\n",
      "Removing Filler Word: 'what about' from Query\n",
      "Removing Filler Word: 'cases in' from Query\n",
      "Partial Match Found: pratt et al v united alaska min co (Abbreviation: pratt v united alaska min co)\n",
      "Partial Match Found: alaska commercial co v raymond (Abbreviation: alaska commercial co v raymond)\n",
      "Partial Match Found: united states v alaska packers assn and babler (Abbreviation: united states v alaska packers assn)\n",
      "Partial Match Found: whitehead v n y and alaska min co (Abbreviation: whitehead v n y alaska min co)\n",
      "Partial Match Found: the alaska gold min co v barbridge et al (Abbreviation: alaska gold min co v barbridge)\n",
      "Partial Match Found: the city of seattle the washington and alaska steamship co claimant (Abbreviation: city of seattle)\n",
      "Partial Match Found: spaulding et al v alaska com co (Abbreviation: spaulding v alaska com co)\n",
      "Partial Match Found: juneau ferry co v alaska steamship co (Abbreviation: juneau ferry co v alaska steamship co)\n",
      "\n",
      "Retrieved Documents for Query: 'What about cases in Alaska?'\n",
      "[8504379, 8504562, 8504808, 8504914, 8505154]\n",
      "\n",
      "Relevant Documents for Query: 'What about cases in Alaska?'\n",
      "[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\n",
      "\n",
      "Processing Query: 'Case on McIntosh?'\n",
      "Removing Filler Word: 'case on' from Query\n",
      "Removing Filler Word: 'case on' from Query\n",
      "Partial Match Found: u s ex rel mcintosh v price et al (Abbreviation: us ex rel mcintosh v price)\n",
      "Partial Match Found: price et al v mcintosh et al (Abbreviation: price v mcintosh)\n",
      "\n",
      "Retrieved Documents for Query: 'Case on McIntosh?'\n",
      "[8504756, 8505061]\n",
      "\n",
      "Relevant Documents for Query: 'Case on McIntosh?'\n",
      "[8504756, 8505061]\n",
      "\n",
      "Mean Average Precision (MAP): 0.8250\n",
      "Query: What about Hillyer?, k=1, Precision@k=1.00, Recall@k=1.00, F1-Score@k=1.00, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: What about Hillyer?, k=3, Precision@k=0.33, Recall@k=1.00, F1-Score@k=0.50, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: What about Hillyer?, k=5, Precision@k=0.20, Recall@k=1.00, F1-Score@k=0.33, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: 1892-03-08, k=1, Precision@k=1.00, Recall@k=1.00, F1-Score@k=1.00, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: 1892-03-08, k=3, Precision@k=0.33, Recall@k=1.00, F1-Score@k=0.50, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: 1892-03-08, k=5, Precision@k=0.20, Recall@k=1.00, F1-Score@k=0.33, nDCG@k=1.00\n",
      "Retrieved Docs: [8504265]\n",
      "Relevant Docs: [8504265]\n",
      "----------------------------------------\n",
      "Query: United, k=1, Precision@k=1.00, Recall@k=0.10, F1-Score@k=0.18, nDCG@k=1.00\n",
      "Retrieved Docs: [8504008]\n",
      "Relevant Docs: [8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\n",
      "----------------------------------------\n",
      "Query: United, k=3, Precision@k=1.00, Recall@k=0.30, F1-Score@k=0.46, nDCG@k=1.00\n",
      "Retrieved Docs: [8504008, 8504265, 8504379]\n",
      "Relevant Docs: [8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\n",
      "----------------------------------------\n",
      "Query: United, k=5, Precision@k=1.00, Recall@k=0.50, F1-Score@k=0.67, nDCG@k=1.00\n",
      "Retrieved Docs: [8504008, 8504265, 8504379, 8504693, 8504808]\n",
      "Relevant Docs: [8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\n",
      "----------------------------------------\n",
      "Query: What about cases in Alaska?, k=1, Precision@k=1.00, Recall@k=0.12, F1-Score@k=0.22, nDCG@k=1.00\n",
      "Retrieved Docs: [8504379]\n",
      "Relevant Docs: [8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\n",
      "----------------------------------------\n",
      "Query: What about cases in Alaska?, k=3, Precision@k=1.00, Recall@k=0.38, F1-Score@k=0.55, nDCG@k=1.00\n",
      "Retrieved Docs: [8504379, 8504562, 8504808]\n",
      "Relevant Docs: [8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\n",
      "----------------------------------------\n",
      "Query: What about cases in Alaska?, k=5, Precision@k=1.00, Recall@k=0.62, F1-Score@k=0.77, nDCG@k=1.00\n",
      "Retrieved Docs: [8504379, 8504562, 8504808, 8504914, 8505154]\n",
      "Relevant Docs: [8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\n",
      "----------------------------------------\n",
      "Query: Case on McIntosh?, k=1, Precision@k=1.00, Recall@k=0.50, F1-Score@k=0.67, nDCG@k=1.00\n",
      "Retrieved Docs: [8504756]\n",
      "Relevant Docs: [8504756, 8505061]\n",
      "----------------------------------------\n",
      "Query: Case on McIntosh?, k=3, Precision@k=0.67, Recall@k=1.00, F1-Score@k=0.80, nDCG@k=1.00\n",
      "Retrieved Docs: [8504756, 8505061]\n",
      "Relevant Docs: [8504756, 8505061]\n",
      "----------------------------------------\n",
      "Query: Case on McIntosh?, k=5, Precision@k=0.40, Recall@k=1.00, F1-Score@k=0.57, nDCG@k=1.00\n",
      "Retrieved Docs: [8504756, 8505061]\n",
      "Relevant Docs: [8504756, 8505061]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def compute_average_precision(retrieved_docs, relevant_docs):\n",
    "    \"\"\"Compute Average Precision for a single query.\"\"\"\n",
    "    relevant_set = set(relevant_docs)\n",
    "    num_relevant = len(relevant_set)\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision_sum = 0.0\n",
    "    relevant_retrieved = 0\n",
    "    for k, doc in enumerate(retrieved_docs, start=1):\n",
    "        if doc in relevant_set:\n",
    "            relevant_retrieved += 1\n",
    "            precision_sum += relevant_retrieved / k\n",
    "\n",
    "    return precision_sum / num_relevant\n",
    "\n",
    "def compute_ndcg(retrieved_docs, relevant_docs, k):\n",
    "    \"\"\"Compute nDCG for a single query.\"\"\"\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    relevant_set = set(relevant_docs)\n",
    "    for i in range(1, k + 1):\n",
    "        if i <= len(retrieved_docs) and retrieved_docs[i - 1] in relevant_set:\n",
    "            dcg += 1 / math.log2(i + 1)\n",
    "        if i <= len(relevant_docs):\n",
    "            idcg += 1 / math.log2(i + 1)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def test_multiple_queries(queries, k_values, metadata_file, embeddings_file):\n",
    "    \"\"\"\n",
    "    Test retrieval and calculate metrics for multiple queries.\n",
    "\n",
    "    Parameters:\n",
    "        queries (list): A list of query strings.\n",
    "        k_values (list): List of k values to calculate precision@k, recall@k, etc.\n",
    "        metadata_file (str): Path to the metadata JSON file.\n",
    "        embeddings_file (str): Path to the embeddings file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nProcessing Query: '{query}'\")\n",
    "        \n",
    "        # preprocessing the query\n",
    "        normalized_query = preprocess_query(query)\n",
    "        #print(f\"Normalized Query: '{normalized_query}'\")\n",
    "\n",
    "        # load\n",
    "        with open(metadata_file, \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "        embeddings = np.load(embeddings_file, allow_pickle=True)\n",
    "\n",
    "        # retrieve documents using colbert_retrieve logic\n",
    "        retrieved_docs, filtered_results = colbert_retrieve(\n",
    "            query, embeddings_file, metadata_file, query_type=\"custom\", top_k=max(k_values)\n",
    "        )\n",
    "\n",
    "        # printing output for retrieved docs\n",
    "        retrieved_doc_ids = [doc[\"id\"] for doc in filtered_results]\n",
    "        print(f\"\\nRetrieved Documents for Query: '{query}'\")\n",
    "        print(retrieved_doc_ids)\n",
    "\n",
    "        # determining relevant documents dynamically\n",
    "        relevant_docs = list(set(\n",
    "            [\n",
    "                doc[\"id\"]\n",
    "                for doc in metadata\n",
    "                if normalized_query in doc.get(\"name\", \"\").lower()\n",
    "                or normalized_query in doc.get(\"abbreviation\", \"\").lower()\n",
    "                or normalized_query in doc.get(\"decision_date\", \"\")\n",
    "                or normalized_query == doc.get(\"normalized_date\", \"\")\n",
    "            ] + retrieved_doc_ids\n",
    "        ))\n",
    "\n",
    "        # showing relevant document output\n",
    "        print(f\"\\nRelevant Documents for Query: '{query}'\")\n",
    "        print(relevant_docs)\n",
    "\n",
    "        # calculating the metrics\n",
    "        for k in k_values:\n",
    "            top_k_docs = retrieved_doc_ids[:k]\n",
    "            relevant_set = set(relevant_docs)\n",
    "\n",
    "            # precision@k\n",
    "            relevant_retrieved = len([doc for doc in top_k_docs if doc in relevant_set])\n",
    "            precision = relevant_retrieved / k if k > 0 else 0.0\n",
    "\n",
    "            # recall@k\n",
    "            recall = relevant_retrieved / len(relevant_set) if len(relevant_set) > 0 else 0.0\n",
    "\n",
    "            # f1-Score@k\n",
    "            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "            # nDCG@k\n",
    "            ndcg = compute_ndcg(retrieved_doc_ids, relevant_docs, k)\n",
    "\n",
    "            # appending the results\n",
    "            all_results.append({\n",
    "                \"query\": query,\n",
    "                \"k\": k,\n",
    "                \"precision@k\": precision,\n",
    "                \"recall@k\": recall,\n",
    "                \"f1_score@k\": f1_score,\n",
    "                \"ndcg@k\": ndcg,\n",
    "                \"retrieved_docs\": top_k_docs,\n",
    "                \"relevant_docs\": relevant_docs,\n",
    "            })\n",
    "\n",
    "    # mean average precision (MAP)\n",
    "    map_score = sum(\n",
    "        compute_average_precision(result[\"retrieved_docs\"], result[\"relevant_docs\"])\n",
    "        for result in all_results if result[\"k\"] == max(k_values)\n",
    "    ) / len(queries)\n",
    "\n",
    "    print(f\"\\nMean Average Precision (MAP): {map_score:.4f}\")\n",
    "\n",
    "    # print the aggregated results\n",
    "    for result in all_results:\n",
    "        print(\n",
    "            f\"Query: {result['query']}, k={result['k']}, \"\n",
    "            f\"Precision@k={result['precision@k']:.2f}, Recall@k={result['recall@k']:.2f}, \"\n",
    "            f\"F1-Score@k={result['f1_score@k']:.2f}, nDCG@k={result['ndcg@k']:.2f}\"\n",
    "        )\n",
    "        print(f\"Retrieved Docs: {result['retrieved_docs']}\")\n",
    "        print(f\"Relevant Docs: {result['relevant_docs']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# example usage\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\"What about Hillyer?\", \"1892-03-08\", \"United\", \"What about cases in Alaska?\", \"Case on McIntosh?\"]\n",
    "    k_values = [1, 3, 5]\n",
    "    metadata_file = \"data/metadata.json\"\n",
    "    embeddings_file = \"data/colbert_embeddings.npy\"\n",
    "\n",
    "    test_multiple_queries(queries, k_values, metadata_file, embeddings_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "470f5c53-e3bc-447f-9059-a5fe9f3ad5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data including all relevant fields\n",
    "enhanced_data = {\n",
    "    \"Query\": [\n",
    "        \"What about Hillyer?\", \"What about Hillyer?\", \"What about Hillyer?\",\n",
    "        \"1892-03-08\", \"1892-03-08\", \"1892-03-08\",\n",
    "        \"United\", \"United\", \"United\",\n",
    "        \"What about cases in Alaska?\", \"What about cases in Alaska?\", \"What about cases in Alaska?\",\n",
    "        \"Case on McIntosh?\", \"Case on McIntosh?\", \"Case on McIntosh?\"\n",
    "    ],\n",
    "    \"k\": [1, 3, 5] * 5,\n",
    "    \"Precision@k\": [1.00, 0.33, 0.20, 1.00, 0.33, 0.20, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.67, 0.40],\n",
    "    \"Recall@k\": [1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.10, 0.30, 0.50, 0.12, 0.38, 0.62, 0.50, 1.00, 1.00],\n",
    "    \"F1-Score@k\": [1.00, 0.50, 0.33, 1.00, 0.50, 0.33, 0.18, 0.46, 0.67, 0.22, 0.55, 0.77, 0.67, 0.80, 0.57],\n",
    "    \"nDCG@k\": [1.00] * 15,\n",
    "    \"Retrieved Docs\": [\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\",\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\",\n",
    "        \"[8504008]\", \"[8504008, 8504265, 8504379]\", \"[8504008, 8504265, 8504379, 8504693, 8504808]\",\n",
    "        \"[8504379]\", \"[8504379, 8504562, 8504808]\", \"[8504379, 8504562, 8504808, 8504914, 8505154]\",\n",
    "        \"[8504756]\", \"[8504756, 8505061]\", \"[8504756, 8505061]\"\n",
    "    ],\n",
    "    \"Relevant Docs\": [\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\",\n",
    "        \"[8504265]\", \"[8504265]\", \"[8504265]\",\n",
    "        \"[8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\",\n",
    "        \"[8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\",\n",
    "        \"[8504008, 8504265, 8504808, 8505995, 8506124, 8504693, 8505366, 8506137, 8505818, 8504379]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8505154, 8505634, 8505700, 8504808, 8504562, 8504914, 8504379, 8505789]\",\n",
    "        \"[8504756, 8505061]\", \"[8504756, 8505061]\", \"[8504756, 8505061]\"\n",
    "    ],\n",
    "    \"Match Type\": [\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Exact\", \"Exact\", \"Exact\",\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Partial\", \"Partial\", \"Partial\",\n",
    "        \"Partial\", \"Partial\", \"Partial\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# creating a df\n",
    "enhanced_metrics_table = pd.DataFrame(enhanced_data)\n",
    "\n",
    "# convert the df to long format\n",
    "long_format_table = enhanced_metrics_table.melt(\n",
    "    id_vars=[\"Query\", \"k\"],  # keeping \"query\" and \"k\" as identifiers\n",
    "    var_name=\"Metric\",       # new column for metric names\n",
    "    value_name=\"Value\"       # new column for metric values\n",
    ")\n",
    "\n",
    "# function to long-format\n",
    "def save_long_table_as_image(df, output_file=\"tables/full_table_long_format.png\"):\n",
    "    # Plot the long-format table\n",
    "    fig, ax = plt.subplots(figsize=(12, 15))  # Adjust size for long-format readability\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(df.columns))))\n",
    "\n",
    "    # png\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    plt.savefig(output_file, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# long-format \n",
    "output_file_long = \"tables/full_table_long_format.png\"\n",
    "save_long_table_as_image(long_format_table, output_file=output_file_long)\n",
    "\n",
    "# Confirm the file has been saved\n",
    "#print(f\"Long-format table saved at: {output_file_long}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4470699b-6b57-44fc-9190-8e615abdeb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>F1-Score@k</th>\n",
       "      <th>nDCG@k</th>\n",
       "      <th>Retrieved Docs</th>\n",
       "      <th>Relevant Docs</th>\n",
       "      <th>Match Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What about Hillyer?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What about Hillyer?</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What about Hillyer?</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1892-03-08</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1892-03-08</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1892-03-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>[8504265]</td>\n",
       "      <td>Exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>United</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504008]</td>\n",
       "      <td>[8504008, 8504265, 8504808, 8505995, 8506124, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504008, 8504265, 8504379]</td>\n",
       "      <td>[8504008, 8504265, 8504808, 8505995, 8506124, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>United</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504008, 8504265, 8504379, 8504693, 8504808]</td>\n",
       "      <td>[8504008, 8504265, 8504808, 8505995, 8506124, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What about cases in Alaska?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504379]</td>\n",
       "      <td>[8505154, 8505634, 8505700, 8504808, 8504562, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What about cases in Alaska?</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504379, 8504562, 8504808]</td>\n",
       "      <td>[8505154, 8505634, 8505700, 8504808, 8504562, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What about cases in Alaska?</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504379, 8504562, 8504808, 8504914, 8505154]</td>\n",
       "      <td>[8505154, 8505634, 8505700, 8504808, 8504562, ...</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Case on McIntosh?</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504756]</td>\n",
       "      <td>[8504756, 8505061]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Case on McIntosh?</td>\n",
       "      <td>3</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504756, 8505061]</td>\n",
       "      <td>[8504756, 8505061]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Case on McIntosh?</td>\n",
       "      <td>5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[8504756, 8505061]</td>\n",
       "      <td>[8504756, 8505061]</td>\n",
       "      <td>Partial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Query  k  Precision@k  Recall@k  F1-Score@k  nDCG@k  \\\n",
       "0           What about Hillyer?  1         1.00      1.00        1.00     1.0   \n",
       "1           What about Hillyer?  3         0.33      1.00        0.50     1.0   \n",
       "2           What about Hillyer?  5         0.20      1.00        0.33     1.0   \n",
       "3                    1892-03-08  1         1.00      1.00        1.00     1.0   \n",
       "4                    1892-03-08  3         0.33      1.00        0.50     1.0   \n",
       "5                    1892-03-08  5         0.20      1.00        0.33     1.0   \n",
       "6                        United  1         1.00      0.10        0.18     1.0   \n",
       "7                        United  3         1.00      0.30        0.46     1.0   \n",
       "8                        United  5         1.00      0.50        0.67     1.0   \n",
       "9   What about cases in Alaska?  1         1.00      0.12        0.22     1.0   \n",
       "10  What about cases in Alaska?  3         1.00      0.38        0.55     1.0   \n",
       "11  What about cases in Alaska?  5         1.00      0.62        0.77     1.0   \n",
       "12            Case on McIntosh?  1         1.00      0.50        0.67     1.0   \n",
       "13            Case on McIntosh?  3         0.67      1.00        0.80     1.0   \n",
       "14            Case on McIntosh?  5         0.40      1.00        0.57     1.0   \n",
       "\n",
       "                                   Retrieved Docs  \\\n",
       "0                                       [8504265]   \n",
       "1                                       [8504265]   \n",
       "2                                       [8504265]   \n",
       "3                                       [8504265]   \n",
       "4                                       [8504265]   \n",
       "5                                       [8504265]   \n",
       "6                                       [8504008]   \n",
       "7                     [8504008, 8504265, 8504379]   \n",
       "8   [8504008, 8504265, 8504379, 8504693, 8504808]   \n",
       "9                                       [8504379]   \n",
       "10                    [8504379, 8504562, 8504808]   \n",
       "11  [8504379, 8504562, 8504808, 8504914, 8505154]   \n",
       "12                                      [8504756]   \n",
       "13                             [8504756, 8505061]   \n",
       "14                             [8504756, 8505061]   \n",
       "\n",
       "                                        Relevant Docs Match Type  \n",
       "0                                           [8504265]    Partial  \n",
       "1                                           [8504265]    Partial  \n",
       "2                                           [8504265]    Partial  \n",
       "3                                           [8504265]      Exact  \n",
       "4                                           [8504265]      Exact  \n",
       "5                                           [8504265]      Exact  \n",
       "6   [8504008, 8504265, 8504808, 8505995, 8506124, ...    Partial  \n",
       "7   [8504008, 8504265, 8504808, 8505995, 8506124, ...    Partial  \n",
       "8   [8504008, 8504265, 8504808, 8505995, 8506124, ...    Partial  \n",
       "9   [8505154, 8505634, 8505700, 8504808, 8504562, ...    Partial  \n",
       "10  [8505154, 8505634, 8505700, 8504808, 8504562, ...    Partial  \n",
       "11  [8505154, 8505634, 8505700, 8504808, 8504562, ...    Partial  \n",
       "12                                 [8504756, 8505061]    Partial  \n",
       "13                                 [8504756, 8505061]    Partial  \n",
       "14                                 [8504756, 8505061]    Partial  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af51695-66e7-4bf4-b5cb-2cfcac7e47d5",
   "metadata": {},
   "source": [
    "The retrieval system demonstrates strong performance across multiple metrics, reflecting its ability to balance precision, recall, and ranking quality. Precision@k highlights the system's effectiveness in retrieving relevant documents, achieving perfect precision (1.00) at k=1 for focused queries like \"What about Hillyer?\" and \"1892-03-08.\" However, precision decreases slightly as k increases, especially for broader queries like \"United,\" where less relevant documents may be included. Recall@k complements this by measuring the proportion of all relevant documents retrieved, improving significantly at higher k-values as more relevant items are captured. The F1-score@k, which balances precision and recall, shows consistent performance, particularly for queries like \"Case on McIntosh?\" (F1-score@3: 0.80), indicating a strong overlap between retrieved and relevant documents. nDCG@k (1.0 for all queries) demonstrates excellent ranking quality, with relevant documents consistently appearing early in the list. The overall Mean Average Precision (MAP) of 0.825 underscores the system's reliability in retrieving and ranking relevant documents across both specific and broad queries. These results highlight the system's ability to handle diverse query types effectively, delivering high relevance and ranking accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
