{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Case Indexing and Querying\n",
    "- Generating embeddings using Hugging Face models\n",
    "- Indexing using FAISS\n",
    "- Querying indexed embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Embedding Generation (Run outside `js2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1.post100)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-12-05 13:49:34.166777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing JSON files\n",
    "json_dir = \"./json\"\n",
    "metadata = []\n",
    "embeddings_list = []\n",
    "\n",
    "# Process JSON files\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(json_dir, file_name)\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract and process opinions\n",
    "        for opinion in data.get(\"casebody\", {}).get(\"opinions\", []):\n",
    "            text = opinion.get(\"text\", \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Split long text into smaller passages\n",
    "            passages = [text[i:i + 300] for i in range(0, len(text), 300)]\n",
    "            for passage in passages:\n",
    "                embedding = embed_text(passage)\n",
    "                embeddings_list.append(embedding)\n",
    "                metadata.append({\"file\": file_name, \"text\": passage})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "## Code review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and metadata saved!\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings and metadata\n",
    "embeddings_array = np.vstack(embeddings_list)\n",
    "np.save(\"embeddings.npy\", embeddings_array)\n",
    "\n",
    "with open(\"metadata_new.json\", \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "    \n",
    "print(\"Embeddings and metadata saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Indexing and Querying (Run inside `js2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m225.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, faiss-cpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
      "cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
      "cupy-cuda12x 12.3.0 requires numpy<1.29,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
      "dask-cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
      "matplotlib 3.8.2 requires numpy<2,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
      "numba 0.57.1+1.g1ff679645 requires numpy<1.25,>=1.21, but you have numpy 2.1.3 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-cpu-1.9.0.post1 numpy-2.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"embeddings.npy\", allow_pickle=True)\n",
    "\n",
    "with open(\"metadata_new.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS index\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FAISS index\n",
    "faiss.write_index(index, \"legal_cases_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "Rank 1: {'file': '0001-01.json', 'text': 'DAWSON, District Judge.\\nPetitioner, by his guardian, ad litem, sets forth that he is unlawfully restrained of his liberty by Lieutenant Commander J. S. Newell, naval officer in charge at this station, and in command of the United States steamer and man-of-war Pinta. He states that he was enlisted in'}\n",
      "Rank 2: {'file': '0001-01.json', 'text': 'f inducing another person to contract with him, he is estopped from afterwards denying it.. See Bigelow on Estoppel, pp. 486, 487.\\nIt follows that the prayer of the petitioner must be denied, and that he be remanded to the custody of Lieutenant Commander Newell and his successors until he is 21 year'}\n",
      "Rank 3: {'file': '0407-01.json', 'text': 'lified terms of the requests. The plaintiff, as a sailor, was amenable to rigid discipline for disobedience of orders. He was injured while discharging a duty to which he had been assigned by his superior officer, and which he was performing under the eye of the master of the ship. Notwithstanding w'}\n",
      "Rank 4: {'file': '0607-01.json', 'text': ' Leaner . 15 00\\'\\nWm. Prichard .\".. 6 OO\\nD. O. Gilman . 15 OO\\nMagnus Norman . 15 00\\'\\nThomas Jackson . 6 00\\nThomas A. Johnson . 6 00\\'\\nA. McClellan . 15 OO\\nL. W. Nestelle . ■ 30 O\\'O\\'\\nA. G. Kingsbury . 30 00\\'\\nA. G. Maddren . 15 00\\nFrank Fogg . 15 00\\nH. B. Strong . 6 00\\nA. H. Strong . 6 00\\nDistributed ..'}\n",
      "Rank 5: {'file': '0114-01.json', 'text': 'v. Babin, 60 U. S. 271, 15 L. Ed. 633; Grand Chute v. Winegar, 92 U. S. 373, 21 L. Ed. 174.\\nIn Parker v. Winnipiseogee, etc., Co., 67 U. S. 545, 17 L. Ed. 333, the Supreme Court of the United States says:\\n“It was urged, at Rearing, as an insuperable objection to tbe relief prayed for, that tbe appel'}\n"
     ]
    }
   ],
   "source": [
    "# Query example\n",
    "query_vector = embeddings[0]\n",
    "k = 5  # Top-5 results\n",
    "distances, indices = index.search(query_vector[np.newaxis, :], k)\n",
    "\n",
    "print(\"Query results:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Rank {i+1}: {metadata[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Querying the Indexed Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35c64ff630046538e85e8d07a0db034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3476713b5a8a4c61a97d3dc5ea88cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eaf78c121c44f639d8cd275579fc071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810b9e0a02fd44949a0a331f884f268c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5667b75baae4d8fbae694f7aee71ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 17:35:25.145434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8228fda18b44e8936f533defad424e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the embedding model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"legal_cases_index.faiss\")\n",
    "with open(\"metadata_new.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    \"\"\"Generate embeddings for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query function\n",
    "def query_index(user_query, index, metadata):\n",
    "    query_embedding = embed_text(user_query)\n",
    "    k = 5\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append({\"rank\": i+1, \"file\": metadata[idx][\"file\"], \"text\": metadata[idx][\"text\"], \"distance\": distances[0][i]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "user_query = \"What legal precedents are there for unlawful restraint?\"\n",
    "results = query_index(user_query, index, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "File: 0154-01.json\n",
      "Text:  such a nature that they cannot be cured by amendment.\n",
      "Sections 121 and 122 of the Code of Civil Procedure (Act June 6, 1900, c. 786, 31 Stat. 351) provide that a motion to vacate the writ of arrest may be made with or without affidavits or other proofs.\n",
      "“If, upon tlie Rearing of such motion, it sha\n",
      "Distance: 22.2808\n",
      "\n",
      "Rank 2:\n",
      "File: 0165-01.json\n",
      "Text: question as to whether or not the defendants were in fact encroaching upon the rights of the plaintiffs, and should, therefore, be restrained. Further than that the court attempted to settle nothing in this case, except what might be in its nature advisory to the defendants in their future operation\n",
      "Distance: 22.6994\n",
      "\n",
      "Rank 3:\n",
      "File: 0311-01.json\n",
      "Text:  is compelled to the conclusion that the defendants should be restrained and enjoined from further continuing the sinking of their shaft, or further interfering in this behalf with the rights of the plaintiff.\n",
      "It may be said in this connection that the situation of the parties plaintiff and defendan\n",
      "Distance: 22.8852\n",
      "\n",
      "Rank 4:\n",
      "File: 0173-01.json\n",
      "Text:  cases where there is no appeal, or other plain, speedy, and adequate remedy, and where the inferior court, officer, or tribunal in the exercise of judicial functions, appears to have exercised such functions erroneously or to have exceeded its or his jurisdiction, to the injury of some substantial \n",
      "Distance: 22.9082\n",
      "\n",
      "Rank 5:\n",
      "File: 0613-01.json\n",
      "Text: erform the duty.\n",
      "The motions in arrest of judgment, for the entry of a judgment of not guilty notwithstanding the verdict, and the motion for a new trial are now hereby overruled, and the defendants, and each of them, will be allowed an exception to this action of the court.\n",
      "Distance: 22.9325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Rank {result['rank']}:\")\n",
    "    print(f\"File: {result['file']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"query_results.txt\", \"w\") as f:\n",
    "    for result in results:\n",
    "        f.write(f\"Rank {result['rank']}:\\n\")\n",
    "        f.write(f\"File: {result['file']}\\n\")\n",
    "        f.write(f\"Text: {result['text']}\\n\")\n",
    "        f.write(f\"Distance: {result['distance']:.4f}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
