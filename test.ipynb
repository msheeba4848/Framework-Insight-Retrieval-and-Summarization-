{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers.git\n",
    "#!pip install datasets\n",
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheebamoghal/Downloads/rag/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration, AutoTokenizer\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "#source new_env/bin/activate\n",
    "model = RagTokenForGeneration.from_pretrained_question_encoder_generator(\"facebook/dpr-question_encoder-single-nq-base\", \"facebook/bart-large\")\n",
    "question_encoder_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "tokenizer = RagTokenizer(question_encoder_tokenizer, generator_tokenizer)\n",
    "model.config.use_dummy_dataset = False\n",
    "model.config.index_name = \"exact\"\n",
    "retriever = RagRetriever(model.config, question_encoder_tokenizer, generator_tokenizer)\n",
    "\n",
    "model.save_pretrained(\"./\")\n",
    "tokenizer.save_pretrained(\"./\")\n",
    "retriever.save_pretrained(\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if 'embedding_model' not in globals():\n",
    "    embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to embed text for retrieval\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = 768  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "metadata = []\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = 'json'\n",
    "\n",
    "# Process and index each JSON file\n",
    "for json_file in os.listdir(json_dir):\n",
    "    with open(os.path.join(json_dir, json_file), 'r') as f:\n",
    "        case_data = json.load(f)\n",
    "    \n",
    "    # Extract and split passages from case body text\n",
    "    for section in case_data['casebody']['opinions']:\n",
    "        text = section['text']\n",
    "        # Split text into smaller passages\n",
    "        passages = [text[i:i+300] for i in range(0, len(text), 300)]\n",
    "        \n",
    "        for passage in passages:\n",
    "            # Embed passage and add to FAISS index\n",
    "            embedding = embed_text(passage)\n",
    "            index.add(embedding)\n",
    "            metadata.append({'file': json_file, 'text': passage})\n",
    "\n",
    "# Save index and metadata\n",
    "faiss.write_index(index, 'legal_cases_index.faiss')\n",
    "with open('metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "# Query and Retrieve Relevant Passages\n",
    "query = \"What is the ruling on minors' ability to enlist in the navy?\"\n",
    "query_embedding = embed_text(query)\n",
    "\n",
    "# Perform retrieval\n",
    "_, I = index.search(query_embedding, k=5)  # Retrieve top 5 results\n",
    "\n",
    "# Retrieve passages based on FAISS indices\n",
    "retrieved_passages = [metadata[i]['text'] for i in I[0]]\n",
    "\n",
    "# Use RAG to generate summary\n",
    "input_ids = rag_tokenizer(query, return_tensors=\"pt\").input_ids\n",
    "generated_ids = rag_model.generate(input_ids, context_input_ids=retrieved_passages)\n",
    "summary = rag_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Queries:\n",
      "1. How does this case interpret 31 Stat. 356?\n",
      "2. Provide a summary of 53 Pac. 536.\n",
      "3. What rights were discussed regarding 81 Fed. 211?\n",
      "4. What was the ruling on 90 Fed. 673?\n",
      "5. Provide a summary of 31 Stat. 383.\n",
      "6. What are the legal principles established in 5 Or. 438?\n",
      "7. Provide a summary of THE CATHERINE SUDDEN.\n",
      "8. What was the ruling on 31 Stat. 528?\n",
      "9. How does this case interpret 23 Stat. 24?\n",
      "10. How does this case interpret 25 L. Ed. 435?\n"
     ]
    }
   ],
   "source": [
    "# Function to generate automated queries\n",
    "def generate_queries(metadata, templates, num_queries=10):\n",
    "    queries = []\n",
    "    for _ in range(num_queries):\n",
    "        case_info = random.choice(metadata)\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        # Use case name or a random topic for the query\n",
    "        if case_info[\"topics\"]:\n",
    "            topic = random.choice(case_info[\"topics\"])\n",
    "        else:\n",
    "            topic = case_info[\"name\"]\n",
    "        \n",
    "        # Generate query\n",
    "        query = template.format(topic)\n",
    "        queries.append(query)\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Generate sample queries\n",
    "sample_queries = generate_queries(metadata, query_templates, num_queries=10)\n",
    "\n",
    "# Print or save the queries\n",
    "print(\"Generated Queries:\")\n",
    "for i, query in enumerate(sample_queries, 1):\n",
    "    print(f\"{i}. {query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a function `query_rag_system` that takes a query and returns a response\n",
    "def query_rag_system(query):\n",
    "    # This function should connect to your RAG system and return a response\n",
    "    # Placeholder for an actual call to RAG model\n",
    "    response = rag_system.generate_response(query)  # Replace with actual API or function call\n",
    "    return response\n",
    "\n",
    "# Run the queries and collect responses\n",
    "responses = []\n",
    "for query in sample_queries:\n",
    "    response = query_rag_system(query)\n",
    "    responses.append({\"query\": query, \"response\": response})\n",
    "\n",
    "# Optionally, save responses for analysis\n",
    "with open(\"rag_responses.json\", \"w\") as f:\n",
    "    json.dump(responses, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
